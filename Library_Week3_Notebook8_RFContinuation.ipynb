{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from numpy.polynomial.polynomial import polyfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep - all code condensed from previous Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2014 (9305, 159) <class 'pandas.core.frame.DataFrame'>\n",
      "_2015 (9251, 159) <class 'pandas.core.frame.DataFrame'>\n",
      "_2016 (9252, 151) <class 'pandas.core.frame.DataFrame'>\n",
      "_2001 (9133, 108) <class 'pandas.core.frame.DataFrame'>\n",
      "_2002 (9141, 107) <class 'pandas.core.frame.DataFrame'>\n",
      "_2003 (9214, 110) <class 'pandas.core.frame.DataFrame'>\n",
      "_2004 (9210, 128) <class 'pandas.core.frame.DataFrame'>\n",
      "_2005 (9201, 131) <class 'pandas.core.frame.DataFrame'>\n",
      "_2006 (9211, 139) <class 'pandas.core.frame.DataFrame'>\n",
      "_2007 (9217, 148) <class 'pandas.core.frame.DataFrame'>\n",
      "_2008 (9284, 150) <class 'pandas.core.frame.DataFrame'>\n",
      "_2009 (9299, 152) <class 'pandas.core.frame.DataFrame'>\n",
      "_2010 (9308, 154) <class 'pandas.core.frame.DataFrame'>\n",
      "_2011 (9315, 157) <class 'pandas.core.frame.DataFrame'>\n",
      "_2012 (9305, 155) <class 'pandas.core.frame.DataFrame'>\n",
      "_2013 (9309, 157) <class 'pandas.core.frame.DataFrame'>\n",
      "_2000 (9078, 108) <class 'pandas.core.frame.DataFrame'>\n",
      "_1992 (8944, 83) <class 'pandas.core.frame.DataFrame'>\n",
      "_1993 (8929, 83) <class 'pandas.core.frame.DataFrame'>\n",
      "_1994 (8920, 83) <class 'pandas.core.frame.DataFrame'>\n",
      "_1995 (8981, 88) <class 'pandas.core.frame.DataFrame'>\n",
      "_1996 (8946, 90) <class 'pandas.core.frame.DataFrame'>\n",
      "_1997 (8968, 96) <class 'pandas.core.frame.DataFrame'>\n",
      "_1998 (8966, 102) <class 'pandas.core.frame.DataFrame'>\n",
      "_1999 (9048, 108) <class 'pandas.core.frame.DataFrame'>\n",
      "Original dfs:\n",
      "_2014 (9305, 159)\n",
      "_2015 (9251, 159)\n",
      "_2016 (9252, 151)\n",
      "_2001 (9133, 108)\n",
      "_2002 (9141, 107)\n",
      "_2003 (9214, 110)\n",
      "_2004 (9210, 128)\n",
      "_2005 (9201, 131)\n",
      "_2006 (9211, 139)\n",
      "_2007 (9217, 148)\n",
      "_2008 (9284, 150)\n",
      "_2009 (9299, 152)\n",
      "_2010 (9308, 154)\n",
      "_2011 (9315, 157)\n",
      "_2012 (9305, 155)\n",
      "_2013 (9309, 157)\n",
      "_2000 (9078, 108)\n",
      "_1992 (8944, 83)\n",
      "_1993 (8929, 83)\n",
      "_1994 (8920, 83)\n",
      "_1995 (8981, 88)\n",
      "_1996 (8946, 90)\n",
      "_1997 (8968, 96)\n",
      "_1998 (8966, 102)\n",
      "_1999 (9048, 108)\n",
      "\n",
      " Reduced dfs:\n",
      "_2014 (9305, 67)\n",
      "_2015 (9251, 70)\n",
      "_2016 (9252, 75)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 65)\n",
      "_2011 (9315, 65)\n",
      "_2012 (9305, 65)\n",
      "_2013 (9309, 66)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n",
      "_1999 (9048, 52)\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATCIR', 'ELMATEXP', 'ENDDATE',\n",
      "       'FCAP_REV', 'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN',\n",
      "       'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME',\n",
      "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID',\n",
      "       'PHONE', 'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'WIFISESS', 'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'EC_LO_OT', 'EC_ST', 'ELECCOLL',\n",
      "       'ELMATCIR', 'ELMATEXP', 'ENDDATE', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE',\n",
      "       'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX',\n",
      "       'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR',\n",
      "       'RSTATUS', 'SALARIES', 'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT',\n",
      "       'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL',\n",
      "       'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF',\n",
      "       'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS', 'WIFISESS', 'YAATTEN', 'YAPRO',\n",
      "       'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'EC_LO_OT', 'EC_ST', 'ELCONT',\n",
      "       'ELECCOLL', 'ELINFO', 'ELMATCIR', 'ELMATEXP', 'ENDDATE', 'FCAP_REV',\n",
      "       'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'PHYSCIR',\n",
      "       'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REAPLOCALE', 'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES',\n",
      "       'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT',\n",
      "       'TOTATTEN', 'TOTCIR', 'TOTCOLL', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM',\n",
      "       'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL',\n",
      "       'VIDEO_PH', 'ATTEND-VISITS', 'WIFISESS', 'YAATTEN', 'YAPRO',\n",
      "       'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'ERES_USR', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'ERES_USR', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ERES_USR', 'ESUBSCRP', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID', 'LIBNAME',\n",
      "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER', 'OBEREG',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'RSTATUS',\n",
      "       'SALARIES', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ERES_USR', 'ESUBSCRP', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE',\n",
      "       'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES', 'STABR', 'STAFFEXP-TOTEXP',\n",
      "       'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ERES_USR', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY',\n",
      "       'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM',\n",
      "       'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'RSTATUS',\n",
      "       'SALARIES', 'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT',\n",
      "       'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL',\n",
      "       'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT',\n",
      "       'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP',\n",
      "       'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP',\n",
      "       'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES',\n",
      "       'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT',\n",
      "       'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM',\n",
      "       'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT',\n",
      "       'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP',\n",
      "       'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP',\n",
      "       'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES',\n",
      "       'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT',\n",
      "       'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM',\n",
      "       'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE',\n",
      "       'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX',\n",
      "       'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR',\n",
      "       'RSTATUS', 'SALARIES', 'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT',\n",
      "       'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL',\n",
      "       'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE',\n",
      "       'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX',\n",
      "       'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR',\n",
      "       'RSTATUS', 'SALARIES', 'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT',\n",
      "       'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL',\n",
      "       'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATEXP', 'ENDDATE', 'FCAP_REV',\n",
      "       'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR',\n",
      "       'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATEXP', 'ENDDATE', 'FCAP_REV',\n",
      "       'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR',\n",
      "       'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATEXP', 'ENDDATE', 'FCAP_REV',\n",
      "       'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR',\n",
      "       'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATCIR', 'ELMATEXP', 'ENDDATE',\n",
      "       'FCAP_REV', 'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN',\n",
      "       'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME',\n",
      "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID',\n",
      "       'PHONE', 'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'ERES_USR', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'FSCSKEY', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'FSCSKEY', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'FSCSKEY', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'ELACCEXP', 'ELMATEXP', 'ELMATS', 'ELSVCACC',\n",
      "       'FSCSKEY', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'ELACCEXP', 'ELMATEXP', 'ELMATS', 'ELSVCACC',\n",
      "       'FSCSKEY', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'ELACCEXP', 'ELMATEXP', 'ELMATS', 'ELSVCACC',\n",
      "       'FSCSKEY', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN',\n",
      "       'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID', 'LIBNAME',\n",
      "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER', 'OBEREG',\n",
      "       'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'ERES_USR', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "_2014 (9305, 67)\n",
      "_2015 (9251, 70)\n",
      "_2016 (9252, 75)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 65)\n",
      "_2011 (9315, 65)\n",
      "_2012 (9305, 65)\n",
      "_2013 (9309, 66)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n",
      "_1999 (9048, 52)\n",
      "_2014 (9305, 69)\n",
      "_2015 (9251, 72)\n",
      "_2016 (9252, 77)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 67)\n",
      "_2011 (9315, 67)\n",
      "_2012 (9305, 67)\n",
      "_2013 (9309, 68)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_1999 (9048, 52)\n",
      "_2014 (9305, 69)\n",
      "_2015 (9251, 72)\n",
      "_2016 (9252, 77)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 67)\n",
      "_2011 (9315, 67)\n",
      "_2012 (9305, 67)\n",
      "_2013 (9309, 68)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n",
      "_1999 (9048, 52)\n",
      "Reduced dfs:\n",
      "_2014 (9305, 69)\n",
      "_2015 (9251, 72)\n",
      "_2016 (9252, 77)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 67)\n",
      "_2011 (9315, 67)\n",
      "_2012 (9305, 67)\n",
      "_2013 (9309, 68)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n",
      "_1999 (9048, 52)\n",
      "\n",
      " Reduced and dropped useless dfs:\n",
      "_2014 (9305, 63)\n",
      "_2015 (9251, 66)\n",
      "_2016 (9252, 70)\n",
      "_2001 (9133, 47)\n",
      "_2002 (9141, 47)\n",
      "_2003 (9214, 48)\n",
      "_2004 (9210, 50)\n",
      "_2005 (9201, 54)\n",
      "_2006 (9211, 55)\n",
      "_2007 (9217, 55)\n",
      "_2008 (9284, 56)\n",
      "_2009 (9299, 58)\n",
      "_2010 (9308, 61)\n",
      "_2011 (9315, 61)\n",
      "_2012 (9305, 61)\n",
      "_2013 (9309, 62)\n",
      "_2000 (9078, 47)\n",
      "_1992 (8944, 39)\n",
      "_1993 (8929, 39)\n",
      "_1994 (8920, 39)\n",
      "_1995 (8981, 44)\n",
      "_1996 (8946, 44)\n",
      "_1997 (8968, 44)\n",
      "_1998 (8966, 46)\n",
      "_1999 (9048, 47)\n",
      "45\n",
      "(array([ 103,  446, 3560, 3782, 4470, 4478, 4739, 4767, 4768, 5177, 5214,\n",
      "       7393, 7395, 7400, 7401, 7403, 7407, 7408, 7409, 7410, 7412, 7414,\n",
      "       7416, 7421, 7422, 7424, 7425, 7427, 7428, 7429, 7431, 7433, 7434,\n",
      "       7435, 7436, 7438, 7441, 7442, 7444, 7445, 7447, 7448, 7449, 7450,\n",
      "       8559]),)\n",
      "45\n",
      "(array([ 103,  446, 3560, 3782, 4470, 4478, 4739, 4767, 4768, 5177, 5214,\n",
      "       7393, 7395, 7400, 7401, 7403, 7407, 7408, 7409, 7410, 7412, 7414,\n",
      "       7416, 7421, 7422, 7424, 7425, 7427, 7428, 7429, 7431, 7433, 7434,\n",
      "       7435, 7436, 7438, 7441, 7442, 7444, 7445, 7447, 7448, 7449, 7450,\n",
      "       8559]),)\n",
      "_2014 (9305, 63)\n",
      "_2015 (9251, 66)\n",
      "_2016 (9252, 70)\n",
      "_2001 (9133, 47)\n",
      "_2002 (9141, 47)\n",
      "_2003 (9214, 48)\n",
      "_2004 (9210, 50)\n",
      "_2005 (9201, 54)\n",
      "_2006 (9211, 55)\n",
      "_2007 (9217, 55)\n",
      "_2008 (9284, 56)\n",
      "_2009 (9299, 58)\n",
      "_2010 (9308, 61)\n",
      "_2011 (9315, 61)\n",
      "_2012 (9305, 61)\n",
      "_2013 (9309, 62)\n",
      "_2000 (9078, 47)\n",
      "_1992 (8944, 39)\n",
      "_1993 (8929, 39)\n",
      "_1994 (8920, 39)\n",
      "_1995 (8981, 44)\n",
      "_1996 (8946, 44)\n",
      "_1997 (8968, 44)\n",
      "_1998 (8966, 46)\n",
      "_1999 (9048, 47)\n"
     ]
    }
   ],
   "source": [
    "##CODE FROM NOTEBOOK: Library_Week2_Notebook4_Strategy#2\n",
    "#(Load and process data created in Library_Week2_Notebook4_Strategy#2)\n",
    "\n",
    "#Load data\n",
    "os.chdir(\"//Users/Olga/Documents/INSIGHT2019/Library data/AllPldData\")\n",
    "Files_in_folder = os.listdir()\n",
    "File_names = ['_2014', '_2015', '_2016', '_2001', '_2002', '_2003', \n",
    "              '_2004','_2005', '_2006', '_2007', '_2008', '_2009', \n",
    "              '_2010', '_2011', '_2012', '_2013', '_2000', '_1992', \n",
    "              '_1993', '_1994', '_1995', '_1996', '_1997', '_1998', '_1999']\n",
    "Files = []\n",
    "\n",
    "for filename in os.listdir():\n",
    "    if filename.endswith('csv'):\n",
    "        Files.append(pd.read_csv(filename, encoding = 'latin-1', low_memory = False))\n",
    "        \n",
    "#create dictionary of libraries labeled by year\n",
    "LibData_dict = {}\n",
    "for i in range(0, len(File_names)):\n",
    "    LibData_dict[File_names[i]] = Files[i]\n",
    "    \n",
    "for k, v in LibData_dict.items():\n",
    "    print(k, v.shape, type(LibData_dict[k]))\n",
    "\n",
    "ColumnsPresent = pd.DataFrame(index=range(0),columns=range(0))\n",
    "\n",
    "for k, v in LibData_dict.items():\n",
    "    dataframe_to_join = pd.DataFrame({k: list(v.columns)})\n",
    "    ColumnsPresent = ColumnsPresent.join(dataframe_to_join, how = 'outer')\n",
    "\n",
    "#Rearrange columns in dataframe. \n",
    "cols = ColumnsPresent.columns.tolist()\n",
    "cols_rearr = cols[17:] + cols[0:17]\n",
    "cols_rearr2 = cols_rearr[0:8] + cols_rearr[24:25] + cols_rearr[11:24] + cols_rearr[8:11]\n",
    "\n",
    "ColumnsPresent_rearr = ColumnsPresent[cols_rearr2]\n",
    "\n",
    "ColumnsPresent_rearr_str = ColumnsPresent_rearr.applymap(str)\n",
    "ColumnsPresent_rearr_str = ColumnsPresent_rearr_str.reset_index(drop = True)\n",
    "\n",
    "#create DF of unique column values\n",
    "UniqueLabels = np.unique(ColumnsPresent_rearr_str.values)\n",
    "LabelsBoolDF = pd.DataFrame(UniqueLabels)\n",
    "\n",
    "#Create DF w boolean values for prensence of each unique value\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "for j in range(len(ColumnsPresent_rearr_str.columns)):\n",
    "    lis = []\n",
    "    for i in range(len(UniqueLabels)):\n",
    "        lis.append(ColumnsPresent_rearr_str.iloc[:, j].isin([UniqueLabels[i]]).any())\n",
    "    LabelsBoolDF[ColumnsPresent_rearr_str.columns[j]] = lis\n",
    "    \n",
    "## Sum all TRUE values. Value of 25 = all true\n",
    "sum_list = []\n",
    "for i in range(LabelsBoolDF.shape[0]):\n",
    "    sum_list.append(sum(LabelsBoolDF.iloc[i, 1:26]))\n",
    "\n",
    "#Label OK in Notes\n",
    "LabelsBoolDF['TRUE SUM'] = sum_list\n",
    "LabelsBoolDF['Notes'] = \"\"\n",
    "LabelsBoolDF.loc[LabelsBoolDF[LabelsBoolDF['TRUE SUM']==25].index, 'Notes'] = \"OK\"\n",
    "\n",
    "#rows in which not all TRUE (meaning this column/data does not exist for all year)\n",
    "MissingData = LabelsBoolDF[LabelsBoolDF['TRUE SUM']!=25].index.tolist()\n",
    "\n",
    "#Column names to keep\n",
    "#LabelsBoolDF.loc[[0, 3:13, 15], 0].tolist()\n",
    "Cols_to_keep_idx = [0]\n",
    "Cols_to_keep_idx.extend(range(3,13))\n",
    "Cols_to_keep_idx.extend((16, 18, 20, 25, 26))\n",
    "Cols_to_keep_idx.extend(range(33, 49))\n",
    "Cols_to_keep_idx.extend((64, 135, 137, 138, 183))\n",
    "Cols_to_keep_idx.extend(range(185, 189))\n",
    "Cols_to_keep_idx.extend(range(191, 199))\n",
    "Cols_to_keep_idx.append(201)\n",
    "Cols_to_keep_idx.extend(range(206, 219))\n",
    "Cols_to_keep_idx.append(220)\n",
    "Cols_to_keep_idx.extend(range(222, 229))\n",
    "Cols_to_keep_idx.extend((230, 231, 237))\n",
    "Cols_to_keep_idx.extend(range(237, 256))\n",
    "Cols_to_keep_idx.extend(range(257, 260))\n",
    "Cols_to_keep_idx.extend((262, 263))\n",
    "\n",
    "Cols_to_keep_in_lib_dfs = LabelsBoolDF.iloc[Cols_to_keep_idx, 0].tolist()\n",
    "\n",
    "#Make dictionaries of smaller dataframe with Cols_to_keep_in_lib_dfs ONLY\n",
    "LibData_dict_reduced = dict()\n",
    "\n",
    "for k, v in LibData_dict.items():\n",
    "    LibData_dict_reduced[k] = pd.DataFrame()\n",
    "    for i in Cols_to_keep_in_lib_dfs:\n",
    "        if i in v:\n",
    "            TempList = list(v[i])\n",
    "            LibData_dict_reduced[k][i] = TempList\n",
    "\n",
    "#Did above loop do what I thought it did?\n",
    "print(\"Original dfs:\")\n",
    "for k, v in LibData_dict.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "print(\"\\n\", \"Reduced dfs:\")  \n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "\n",
    "#Combine col names that mean same thing in different years into single \n",
    "Names_to_combine = [\"ATTEND\", \"VISITS\",  \"C_LEGBAS\", \"C_LEGBASE\", \"DUPLI\", \"HRS_OPEN\", \"KIDATTEN\", \"KIDATTEND\", \n",
    "                    \"LIBRARIAN\", \"LIBRARIA\", \"POPU\", \"POPU_LSA\", \"POPU_UND\", \"POPU_UNDUP\", \"REFERENC\", \"REFERENCE\", \n",
    "                    \"STAFFEXP\", \"TOTEXP\", \"SUBSCRIP\", \"SUBSCRIPT\", \"TOTEXPCO\", \"TOTEXPCOL\", \n",
    "                    \"TOTOPEXP\", \"TOTOPEXP1\", \"TOTPEMP\", \"TOTSTAFF\", \"ZIP\", \"ZIP1\"]\n",
    "\n",
    "for i in range(0, len(Names_to_combine)-1, 2):\n",
    "    for k, v in LibData_dict_reduced.items(): \n",
    "        if Names_to_combine[i] in v:\n",
    "            name = Names_to_combine[i]\n",
    "            newname = Names_to_combine[i] + \"-\" + Names_to_combine[i+1]\n",
    "            v = v.rename(columns = {name: newname}, inplace = True)\n",
    "        elif Names_to_combine[i+1] in v:\n",
    "            name = Names_to_combine[i + 1]\n",
    "            newname = Names_to_combine[i] + \"-\" + Names_to_combine[i+1]\n",
    "            v = v.rename(columns = {name : newname}, inplace = True)\n",
    "\n",
    "#Check that above code worked - names changes\n",
    "for k, v in LibData_dict_reduced.items():\n",
    "    print(v.columns)\n",
    "\n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "\n",
    "#Columns to add to DFs\n",
    "\"AUDIO is divided into AUDIO_PH, AUDIO_DL starting 2010. Source: 2010 Documentation.\"\n",
    "\"VIDEO was later split into VIDEO_DL and VIDEO_PH.\"\n",
    "\n",
    "for k, v in LibData_dict_reduced.items(): \n",
    "    if \"AUDIO_DL\" in v:\n",
    "        v[\"AUDIO\"] = v['AUDIO_DL'] + v['AUDIO_PH']\n",
    "    if \"VIDEO_DL\" in v:\n",
    "        v[\"VIDEO\"] = v['VIDEO_DL'] + v['VIDEO_PH']\n",
    "\n",
    "#Check that # columns in some dataframes increased by 2 (expected)\n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "    \n",
    "#Create DF w boolean values for prensence of each unique value in REDUCED dataframes\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "\n",
    "#Get all unique columns present in LibData_dict_reduced dataframes. \n",
    "ColumnsPresent_reduced = pd.DataFrame(index=range(0),columns=range(0))\n",
    "\n",
    "for k, v in LibData_dict_reduced.items():\n",
    "    dataframe_to_join_reduced = pd.DataFrame({k: list(v.columns)})\n",
    "    ColumnsPresent_reduced = ColumnsPresent_reduced.join(dataframe_to_join_reduced, how = 'outer')\n",
    "\n",
    "ColumnsPresent_reduced_str = ColumnsPresent_reduced.applymap(str)\n",
    "ColumnsPresent_reduced_str = ColumnsPresent_reduced_str.reset_index(drop = True)\n",
    "    \n",
    "UniqueLabels_reduced = np.unique(ColumnsPresent_reduced_str.values)\n",
    "LabelsBoolDF_reduced = pd.DataFrame(UniqueLabels_reduced)\n",
    "\n",
    "#Create DF w boolean values for prensence of each unique value\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "for j in range(len(ColumnsPresent_reduced_str.columns)):\n",
    "    lis = []\n",
    "    for i in range(len(UniqueLabels_reduced)):\n",
    "        lis.append(ColumnsPresent_reduced_str.iloc[:, j].isin([UniqueLabels_reduced[i]]).any())\n",
    "    LabelsBoolDF_reduced[ColumnsPresent_reduced_str.columns[j]] = lis\n",
    "\n",
    "## Sum all TRUE values. Value of 25 = all true\n",
    "sum_list_reduced = []\n",
    "for i in range(LabelsBoolDF_reduced.shape[0]):\n",
    "    sum_list_reduced.append(sum(LabelsBoolDF_reduced.iloc[i, 1:26]))\n",
    "\n",
    "#Label OK in Notes\n",
    "LabelsBoolDF_reduced['TRUE SUM'] = sum_list_reduced\n",
    "LabelsBoolDF_reduced['Notes'] = \"\"\n",
    "LabelsBoolDF_reduced.loc[LabelsBoolDF_reduced[LabelsBoolDF_reduced['TRUE SUM']==25].index, 'Notes'] = \"OK\"\n",
    "\n",
    "All_Cols_in_Reduced_dfs = list(LabelsBoolDF_reduced[0])\n",
    "All_Cols_in_Reduced_dfs\n",
    "\n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "    \n",
    "#Delete columns deemed NOT useful for downstream analysis\n",
    "NonUsefulColstoDelete = ['ENDDATE', 'LIBID', 'PHONE', 'POPU_UND-POPU_UNDUP', 'RSTATUS', 'TOTCOLL', 'ZIP-ZIP1']\n",
    "\n",
    "LibData_dict_reduced_dropuseless = copy.deepcopy(LibData_dict_reduced)\n",
    "\n",
    "for k, v in LibData_dict_reduced_dropuseless.items(): \n",
    "    for i in range(len(NonUsefulColstoDelete)):\n",
    "        if NonUsefulColstoDelete[i] in v:\n",
    "            v.drop([NonUsefulColstoDelete[i]], axis = 1, inplace = True)\n",
    "            \n",
    "#Check quickly whether columns were dropped\n",
    "print(\"Reduced dfs:\")\n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "\n",
    "print(\"\\n\", \"Reduced and dropped useless dfs:\")\n",
    "for k, v in LibData_dict_reduced_dropuseless.items():    \n",
    "    print(k, v.shape)\n",
    "\n",
    "#Get rid of negative values (replace w NaN)\n",
    "#For all DFs, replace (-1, -3, -9) values with NaN. \n",
    "LibData_dict_reduced_dropuseless_NegRepWNaN = copy.deepcopy(LibData_dict_reduced_dropuseless)\n",
    "\n",
    "for k, v in LibData_dict_reduced_dropuseless_NegRepWNaN.items():\n",
    "    v[(v == -1) | (v == -3) | (v == -9)] = np.nan\n",
    "    \n",
    "#See if this worked (looks like it did)\n",
    "print(sum(LibData_dict_reduced_dropuseless['_2014']['AUDIO_DL'] < 0))\n",
    "print(np.where((LibData_dict_reduced_dropuseless['_2014']['AUDIO_DL'] < 0) == True))\n",
    "\n",
    "print(sum(LibData_dict_reduced_dropuseless_NegRepWNaN['_2014']['AUDIO_DL'].isnull()))\n",
    "print(np.where(LibData_dict_reduced_dropuseless_NegRepWNaN['_2014']['AUDIO_DL'].isnull()))\n",
    "\n",
    "for k, v in LibData_dict_reduced_dropuseless_NegRepWNaN.items():\n",
    "    print(k, v.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2014 (9305, 64)\n",
      "_2015 (9251, 67)\n",
      "_2016 (9252, 71)\n",
      "_2001 (9133, 48)\n",
      "_2002 (9141, 48)\n",
      "_2003 (9214, 49)\n",
      "_2004 (9210, 51)\n",
      "_2005 (9201, 55)\n",
      "_2006 (9211, 56)\n",
      "_2007 (9217, 56)\n",
      "_2008 (9284, 57)\n",
      "_2009 (9299, 59)\n",
      "_2010 (9308, 62)\n",
      "_2011 (9315, 62)\n",
      "_2012 (9305, 62)\n",
      "_2013 (9309, 63)\n",
      "_2000 (9078, 48)\n",
      "_1992 (8944, 40)\n",
      "_1993 (8929, 40)\n",
      "_1994 (8920, 40)\n",
      "_1995 (8981, 45)\n",
      "_1996 (8946, 45)\n",
      "_1997 (8968, 45)\n",
      "_1998 (8966, 47)\n",
      "_1999 (9048, 48)\n",
      "_2014 (9305, 64)\n",
      "_2014 (9259, 64)\n",
      "_2015 (9251, 67)\n",
      "_2015 (9231, 67)\n",
      "_2016 (9252, 71)\n",
      "_2016 (9234, 71)\n",
      "_2001 (9133, 48)\n",
      "_2001 (9131, 48)\n",
      "_2002 (9141, 48)\n",
      "_2002 (9138, 48)\n",
      "_2003 (9214, 49)\n",
      "_2003 (9214, 49)\n",
      "_2004 (9210, 51)\n",
      "_2004 (9207, 51)\n",
      "_2005 (9201, 55)\n",
      "_2005 (9198, 55)\n",
      "_2006 (9211, 56)\n",
      "_2006 (9208, 56)\n",
      "_2007 (9217, 56)\n",
      "_2007 (9214, 56)\n",
      "_2008 (9284, 57)\n",
      "_2008 (9257, 57)\n",
      "_2009 (9299, 59)\n",
      "_2009 (9256, 59)\n",
      "_2010 (9308, 62)\n",
      "_2010 (9273, 62)\n",
      "_2011 (9315, 62)\n",
      "_2011 (9275, 62)\n",
      "_2012 (9305, 62)\n",
      "_2012 (9265, 62)\n",
      "_2013 (9309, 63)\n",
      "_2013 (9263, 63)\n",
      "_2000 (9078, 48)\n",
      "_2000 (9078, 48)\n",
      "_1992 (8944, 40)\n",
      "_1992 (8944, 40)\n",
      "_1993 (8929, 40)\n",
      "_1993 (8929, 40)\n",
      "_1994 (8920, 40)\n",
      "_1994 (8920, 40)\n",
      "_1995 (8981, 45)\n",
      "_1995 (8981, 45)\n",
      "_1996 (8946, 45)\n",
      "_1996 (8946, 45)\n",
      "_1997 (8968, 45)\n",
      "_1997 (8968, 45)\n",
      "_1998 (8966, 47)\n",
      "_1998 (8966, 47)\n",
      "_1999 (9048, 48)\n",
      "_1999 (9048, 48)\n"
     ]
    }
   ],
   "source": [
    "##CODE FROM NOTEBOOK: Library_Week2_Notebook4_Strategy#2\n",
    "#(Load and process data created in Library_Week2_Notebook4_Strategy#2)\n",
    "\n",
    "#Add usage column to each library\n",
    "for k, v in LibData_dict_reduced_dropuseless_NegRepWNaN.items():\n",
    "    v['Usage'] = v['ATTEND-VISITS']/v['POPU-POPU_LSA']\n",
    "    \n",
    "#Check that column was added\n",
    "for k, v in LibData_dict_reduced_dropuseless_NegRepWNaN.items():\n",
    "    print(k, v.shape)\n",
    "    \n",
    "#Remove all rows (libraries) for which Usage data missing for any window!\n",
    "LibwUsage = copy.deepcopy(LibData_dict_reduced_dropuseless_NegRepWNaN)\n",
    "for k, v in LibwUsage.items():\n",
    "    NullInd = v[v.Usage.isnull()].index\n",
    "    print(k, v.shape)\n",
    "    v = v.drop(NullInd, axis = 0)\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CODE FROM NOTEBOOK: Library_Week2_Notebook4_Strategy#2\n",
    "#(Load and process data created in Library_Week2_Notebook4_Strategy#2)\n",
    "\n",
    "#Create DF w boolean values for prensence of each unique value in REDUCED dataframes\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "\n",
    "#Get all unique columns present in LibData_dict_reduced dataframes. \n",
    "ColumnsPresent_reduced = pd.DataFrame(index=range(0),columns=range(0))\n",
    "\n",
    "for k, v in LibwUsage .items():\n",
    "    dataframe_to_join_reduced = pd.DataFrame({k: list(v.columns)})\n",
    "    ColumnsPresent_reduced = ColumnsPresent_reduced.join(dataframe_to_join_reduced, how = 'outer')\n",
    "    \n",
    "UniqueLabels_reduced = np.unique(ColumnsPresent_reduced_str.values)\n",
    "LabelsBoolDF_reduced = pd.DataFrame(UniqueLabels_reduced)\n",
    "\n",
    "#Create DF w boolean values for prensence of each unique value\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "for j in range(len(ColumnsPresent_reduced_str.columns)):\n",
    "    lis = []\n",
    "    for i in range(len(UniqueLabels_reduced)):\n",
    "        lis.append(ColumnsPresent_reduced_str.iloc[:, j].isin([UniqueLabels_reduced[i]]).any())\n",
    "    LabelsBoolDF_reduced[ColumnsPresent_reduced_str.columns[j]] = lis\n",
    "    \n",
    "#Reorder columns\n",
    "colnames = LabelsBoolDF_reduced.columns.tolist()\n",
    "col_order = colnames[0:1] + colnames[18:26] + colnames[17:18] + colnames[4:17] + colnames[1:4]\n",
    "LabelsBoolDF_reduced = LabelsBoolDF_reduced[col_order]\n",
    "\n",
    "## Sum all TRUE values. Value of 25 = all true\n",
    "sum_list_reduced_all = []\n",
    "for i in range(LabelsBoolDF_reduced.shape[0]):\n",
    "    sum_list_reduced_all.append(sum(LabelsBoolDF_reduced.iloc[i, 1:26]))\n",
    "    \n",
    "sum_list_reduced_ten = []\n",
    "for i in range(LabelsBoolDF_reduced.shape[0]):\n",
    "    sum_list_reduced_ten.append(sum(LabelsBoolDF_reduced.iloc[i, 15:26]))\n",
    "    \n",
    "sum_list_reduced_six = []\n",
    "for i in range(LabelsBoolDF_reduced.shape[0]):\n",
    "    sum_list_reduced_six.append(sum(LabelsBoolDF_reduced.iloc[i, 19:26]))\n",
    "\n",
    "#Label OK in Notes\n",
    "LabelsBoolDF_reduced['All_Years_Present'] = sum_list_reduced_all\n",
    "LabelsBoolDF_reduced['2006_2016_Present'] = sum_list_reduced_ten\n",
    "LabelsBoolDF_reduced['2010_2016_Present'] = sum_list_reduced_six\n",
    "\n",
    "#Based on this, I will use 2010-2016 DF. This has the most values.\n",
    "LabelsBoolDF_reduced.loc[29: ,:]\n",
    "\n",
    "#Rename first column\n",
    "LabelsBoolDF_reduced.rename(columns = {LabelsBoolDF_reduced.columns[0] : \"Col\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2014 (9305, 85)\n",
      "_2015 (9251, 85)\n",
      "_2016 (9252, 85)\n",
      "_2001 (9133, 85)\n",
      "_2002 (9141, 85)\n",
      "_2003 (9214, 85)\n",
      "_2004 (9210, 85)\n",
      "_2005 (9201, 85)\n",
      "_2006 (9211, 85)\n",
      "_2007 (9217, 85)\n",
      "_2008 (9284, 85)\n",
      "_2009 (9299, 85)\n",
      "_2010 (9308, 85)\n",
      "_2011 (9315, 85)\n",
      "_2012 (9305, 85)\n",
      "_2013 (9309, 85)\n",
      "_2000 (9078, 85)\n",
      "_1992 (8944, 85)\n",
      "_1993 (8929, 85)\n",
      "_1994 (8920, 85)\n",
      "_1995 (8981, 85)\n",
      "_1996 (8946, 85)\n",
      "_1997 (8968, 85)\n",
      "_1998 (8966, 85)\n",
      "_1999 (9048, 85)\n"
     ]
    }
   ],
   "source": [
    "##CODE FROM NOTEBOOK: Library_Week2_Notebook4_Strategy#2\n",
    "#(Load and process data created in Library_Week2_Notebook4_Strategy#2)\n",
    "\n",
    "#Make sure each df has all 83 columns. If column did not exist, add this column with NaN\n",
    "Unique_col_names_list = list(LabelsBoolDF_reduced['Col'])\n",
    "\n",
    "LibwUsage_2 = copy.deepcopy(LibwUsage)\n",
    "\n",
    "for k, v in LibwUsage_2.items():\n",
    "    for i in range(len(Unique_col_names_list)):\n",
    "        if Unique_col_names_list[i] not in v:\n",
    "            v[Unique_col_names_list[i]] = np.nan\n",
    "\n",
    "for k, v in LibwUsage_2.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate DFs, delete some non-useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65045, 85)\n",
      "Columns to delete: ['EC_LO_OT', 'EC_ST', 'ELACCEXP', 'ELCONT', 'ELECCOLL', 'ELINFO', 'ELMATCIR', 'ELMATS', 'ELSVCACC', 'ERES_USR', 'ESUBSCRP', 'INETACC', 'PHYSCIR', 'REAPLOCALE', 'TOTCOLL', 'WIFISESS']\n",
      "(65045, 85)\n",
      "(65045, 69)\n",
      "(65045, 62)\n",
      "(65045, 62) (65045, 60) (65045, 54)\n"
     ]
    }
   ],
   "source": [
    "#Concatenate 2010-2016\n",
    "Concat201116 = pd.concat([LibwUsage_2['_2010'], LibwUsage_2['_2011'], LibwUsage_2['_2012'], LibwUsage_2['_2013'], LibwUsage_2['_2014'], LibwUsage_2['_2015'], LibwUsage_2['_2016']], axis=0, sort = False, join = \"outer\")\n",
    "print(Concat201116.shape)\n",
    "\n",
    "#rename first col\n",
    "LabelsBoolDF_reduced.rename(columns = {LabelsBoolDF_reduced.columns[0] : \"Col\"}, inplace = True)\n",
    "\n",
    "#Which features are not present in all 7 years?\n",
    "Index = LabelsBoolDF_reduced[LabelsBoolDF_reduced['2010_2016_Present'] != 7].index\n",
    "\n",
    "#Delete these features from DF.\n",
    "Cols_to_delete = LabelsBoolDF_reduced.loc[Index, 'Col']\n",
    "Cols_to_del_list = list(Cols_to_delete[:-1])\n",
    "print(\"Columns to delete:\", Cols_to_del_list)\n",
    "Concat201116_INCOMPLETECOLSDEL = Concat201116.drop(columns = Cols_to_del_list)\n",
    "\n",
    "print(Concat201116.shape)\n",
    "print(Concat201116_INCOMPLETECOLSDEL.shape)\n",
    "\n",
    "#Nonuseful columns to delete from above:\n",
    "NonUsefulColstoDelete = ['ENDDATE', 'LIBID', 'PHONE', 'POPU_UND-POPU_UNDUP', 'RSTATUS', 'ZIP-ZIP1', 'nan']\n",
    "Concat201116_INCOMPLETECOLSDEL2 = Concat201116_INCOMPLETECOLSDEL.drop(columns = NonUsefulColstoDelete)\n",
    "print(Concat201116_INCOMPLETECOLSDEL2.shape)\n",
    "\n",
    "Concat201116_INCOMPLETECOLSDEL2.columns\n",
    "\n",
    "#I SHOULD CHECK THIS MORE CAREFULLY ON FRI!!\n",
    "\n",
    "#Columns to move to front, not use \n",
    "'ADDRESS', 'ATTEND-VISITS', 'CITY', 'CNTY', 'FSCSKEY', 'LIBNAME',   \n",
    "\n",
    "#Columns to delete prior to analysis\n",
    "'AUDIO_DL', 'AUDIO_PH'\n",
    "\n",
    "#Columns to categorize -- DELETE FOR NOW, but for tomorrow may was to categorize\n",
    "'GEOCODE', 'LOCALE', 'OBEREG', 'STABR', 'C_LEGBAS-C_LEGBASE'\n",
    "\n",
    "#Delete 'AUDIO_DL', 'AUDIO_PH'\n",
    "Concat201116_INCOMPLETECOLSDEL3 = Concat201116_INCOMPLETECOLSDEL2.drop(columns = ['AUDIO_DL', 'AUDIO_PH'])\n",
    "#Put 'ADDRESS', 'ATTEND-VISITS', 'CITY', 'CNTY', 'FSCSKEY', 'LIBNAME' in dif DF\n",
    "Location_Detail_DF = Concat201116_INCOMPLETECOLSDEL2[['ADDRESS', 'ATTEND-VISITS', 'CITY', 'CNTY', 'FSCSKEY', 'LIBNAME']]\n",
    "\n",
    "#NOW delete from concatDF\n",
    "Concat201116_INCOMPLETECOLSDEL4 = Concat201116_INCOMPLETECOLSDEL3.drop(columns = ['ADDRESS', 'ATTEND-VISITS', 'CITY', 'CNTY', 'FSCSKEY', 'LIBNAME'])\n",
    "\n",
    "#Do NOT make location columns into dummy variables. We will delete these anyways.\n",
    "#Concat_w_dummies = pd.get_dummies(Concat201116_INCOMPLETECOLSDEL4, columns=['GEOCODE', 'LOCALE', 'OBEREG', 'STABR', 'C_LEGBAS-C_LEGBASE'], prefix=['GEOCODE', 'LOCALE', 'OBEREG', 'STABR', 'C_LEGBAS-C_LEGBASE'])\n",
    "\n",
    "print(Concat201116_INCOMPLETECOLSDEL2.shape, Concat201116_INCOMPLETECOLSDEL3.shape, Concat201116_INCOMPLETECOLSDEL4.shape)\n",
    "\n",
    "#NOW HAVE 2 DFs:\n",
    "#Concat201116_INCOMPLETECOLSDEL4\n",
    "#Location_Detail_DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for dataframe to run in RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE ALL USAGE FEATURES (EXCEPT WHAT I AM TRYING TO PREDICT). \n",
    "#GET RID OF VIDEO_DL, VIDEO_PH for now too\n",
    "ReducedDF = Concat201116_INCOMPLETECOLSDEL4.drop(columns = ['GPTERMS', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'PITUSR', 'REGBOR', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR', 'YAATTEN', 'REFERENC-REFERENCE', 'GEOCODE', 'LOCALE', 'OBEREG', 'STABR', 'C_LEGBAS-C_LEGBASE', 'VIDEO_DL', 'VIDEO_PH'])\n",
    "\n",
    "#Leave 'Usage', 'POPU-POPU_LSA', so that I can divide!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB',\n",
       "       'EBOOK', 'ELMATEXP', 'FCAP_REV', 'DUPLI-HRS_OPEN', 'KIDPRO',\n",
       "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
       "       'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID',\n",
       "       'POPU-POPU_LSA', 'PRMATEXP', 'SALARIES', 'SCAP_REV', 'STAFFEXP-TOTEXP',\n",
       "       'STGVT', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
       "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'YAPRO', 'AUDIO', 'VIDEO', 'Usage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReducedDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65045, 37)\n",
      "(65045, 6)\n",
      "21142\n",
      "(43903, 37) (43903, 6)\n",
      "Int64Index([37237, 37239, 37242, 37243, 37244, 37245, 37247, 37248, 37250,\n",
      "            37251,\n",
      "            ...\n",
      "            46487, 46488, 46490, 46499, 46501, 46502, 46503, 46510, 46516,\n",
      "            46517],\n",
      "           dtype='int64', length=3309)\n",
      "Int64Index([37237, 37239, 37242, 37243, 37244, 37245, 37247, 37248, 37250,\n",
      "            37251,\n",
      "            ...\n",
      "            46487, 46488, 46490, 46499, 46501, 46502, 46503, 46510, 46516,\n",
      "            46517],\n",
      "           dtype='int64', length=3309)\n",
      "Int64Index([37237, 37239, 37242, 37243, 37244, 37245, 37247, 37248, 37250,\n",
      "            37251,\n",
      "            ...\n",
      "            46487, 46488, 46490, 46499, 46501, 46502, 46503, 46510, 46516,\n",
      "            46517],\n",
      "           dtype='int64', length=3309)\n",
      "Int64Index([37237, 37239, 37242, 37243, 37244, 37245, 37247, 37248, 37250,\n",
      "            37251,\n",
      "            ...\n",
      "            46487, 46488, 46490, 46499, 46501, 46502, 46503, 46510, 46516,\n",
      "            46517],\n",
      "           dtype='int64', length=3309)\n",
      "3309\n",
      "3309\n",
      "3309\n",
      "(43903, 37) (43903, 6)\n",
      "(40594, 37) (40594, 6)\n"
     ]
    }
   ],
   "source": [
    "#Now, get rid of NaN, spaces\n",
    "#First, NaN\n",
    "print(ReducedDF.shape)\n",
    "print(Location_Detail_DF.shape)\n",
    "\n",
    "ReducedDF = ReducedDF.reset_index(drop = True)\n",
    "Location_Detail_DF = Location_Detail_DF.reset_index(drop = True)\n",
    "\n",
    "#Get null rows\n",
    "print(len(pd.isnull(ReducedDF).any(1).nonzero()[0]))\n",
    "idx = pd.isnull(ReducedDF).any(1).nonzero()[0]\n",
    "\n",
    "#Delete_row_w_any_NaN\n",
    "ReducedDF_noNaN = ReducedDF.drop(idx)\n",
    "Location_Detail_DF_noNaN = Location_Detail_DF.drop(idx)\n",
    "\n",
    "print(ReducedDF_noNaN.shape, Location_Detail_DF_noNaN.shape)\n",
    "\n",
    "#Second, spaces\n",
    "#4 columns - 'OTHOPEXP', 'SALARIES', 'STAFFEXP-TOTEXP', 'BENEFIT' - are objects, meaning some balues are non-numeric\n",
    "#It looks like some are left blank.\n",
    "print(ReducedDF_noNaN[ReducedDF_noNaN.OTHOPEXP == ' '].index)\n",
    "print(ReducedDF_noNaN[ReducedDF_noNaN.SALARIES == ' '].index)\n",
    "print(ReducedDF_noNaN[ReducedDF_noNaN['STAFFEXP-TOTEXP'] == ' '].index)\n",
    "print(ReducedDF_noNaN[ReducedDF_noNaN.BENEFIT == ' '].index)\n",
    "\n",
    "#Are all indices the same? YES. \n",
    "print(len(ReducedDF_noNaN[ReducedDF_noNaN.OTHOPEXP == ' '].index.intersection(ReducedDF_noNaN[ReducedDF_noNaN.SALARIES == ' '].index)))\n",
    "print(len(ReducedDF_noNaN[ReducedDF_noNaN.OTHOPEXP == ' '].index.intersection(ReducedDF_noNaN[ReducedDF_noNaN['STAFFEXP-TOTEXP'] == ' '].index)))\n",
    "print(len(ReducedDF_noNaN[ReducedDF_noNaN.OTHOPEXP == ' '].index.intersection(ReducedDF_noNaN[ReducedDF_noNaN.BENEFIT == ' '].index)))\n",
    "\n",
    "IndexToDropSpaces = ReducedDF_noNaN[ReducedDF_noNaN.OTHOPEXP == ' '].index\n",
    "\n",
    "ReducedDF_noNaN_noSpace = ReducedDF_noNaN.drop(IndexToDropSpaces)\n",
    "Location_Detail_DF_noNaN_noSpace = Location_Detail_DF_noNaN.drop(IndexToDropSpaces)\n",
    "print(ReducedDF_noNaN.shape, Location_Detail_DF_noNaN.shape)\n",
    "print(ReducedDF_noNaN_noSpace.shape, Location_Detail_DF_noNaN_noSpace.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 56\n",
      "(40594, 37) (40538, 37)\n",
      "(40594, 6) (40538, 6)\n"
     ]
    }
   ],
   "source": [
    "### Remove Outliers, defined as anything over mean+5*SD\n",
    "\n",
    "ReducedDF_noNaN_noSpace = ReducedDF_noNaN_noSpace.reset_index(drop = True)\n",
    "Location_Detail_DF_noNaN_noSpace = Location_Detail_DF_noNaN_noSpace.reset_index(drop = True)\n",
    "\n",
    "mean_plus_5xSD = ReducedDF_noNaN_noSpace.Usage.mean() + 5*ReducedDF_noNaN_noSpace.Usage.std()\n",
    "OutlierIdx = ReducedDF_noNaN_noSpace[ReducedDF_noNaN_noSpace.Usage > mean_plus_5xSD].index\n",
    "print(\"Number of outliers:\", ReducedDF_noNaN_noSpace.loc[OutlierIdx, ['Usage']].shape[0])\n",
    "      \n",
    "ReducedDF_noNaN_noSpace_DropOutliers=ReducedDF_noNaN_noSpace.drop(OutlierIdx)\n",
    "Location_Detail_DF_noNaN_noSpace_DropOutliers=Location_Detail_DF_noNaN_noSpace.drop(OutlierIdx)\n",
    "\n",
    "print(ReducedDF_noNaN_noSpace.shape, ReducedDF_noNaN_noSpace_DropOutliers.shape)\n",
    "print(Location_Detail_DF_noNaN_noSpace.shape, Location_Detail_DF_noNaN_noSpace_DropOutliers.shape)\n",
    "\n",
    "#NOW the two DFs to work with is:\n",
    "#ReducedDF_noNaN_noSpace_DropOutliers\n",
    "#Location_Detail_DF_noNaN_noSpace_DropOutliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Dataframe to run in RF 2: Divide columns that make sense by population served"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide everything by population\n",
    "ReducedDF_noNaN_noSpace_DropOutliers_POPDIV = copy.deepcopy(ReducedDF_noNaN_noSpace_DropOutliers)\n",
    "\n",
    "cols_to_divide_by_pop = ['BENEFIT','BKMOB','BKVOL','BRANLIB','CAPITAL','CAP_REV','CENTLIB','EBOOK','ELMATEXP',\n",
    " 'FCAP_REV','KIDPRO','LIBRARIAN-LIBRARIA','LOANFM','LOANTO','LOCGVT','MASTER',\n",
    " 'OCAP_REV','OTHINCM','OTHMATEX','OTHOPEXP','OTHPAID',\n",
    " 'PRMATEXP','SALARIES','SCAP_REV','STAFFEXP-TOTEXP','STGVT','TOTEXPCO-TOTEXPCOL',\n",
    " 'TOTINCM','TOTOPEXP-TOTOPEXP1','TOTPRO','TOTPEMP-TOTSTAFF','YAPRO','AUDIO','VIDEO']\n",
    "\n",
    "\n",
    "for col in cols_to_divide_by_pop:\n",
    "    ReducedDF_noNaN_noSpace_DropOutliers_POPDIV[col] = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV[col].astype(float)\n",
    "    ReducedDF_noNaN_noSpace_DropOutliers_POPDIV[col] = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV[col].astype(float)/ReducedDF_noNaN_noSpace_DropOutliers_POPDIV['POPU-POPU_LSA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get random 20% of libs to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GET random set of libraries to remove\n",
    "Location_Detail_DF_noNaN_noSpace_DropOutliers = Location_Detail_DF_noNaN_noSpace_DropOutliers.reset_index(drop=True)\n",
    "ReducedDF_noNaN_noSpace_DropOutliers_POPDIV = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV.reset_index(drop=True)\n",
    "\n",
    "UniqueFSCS = set(Location_Detail_DF_noNaN_noSpace_DropOutliers.loc[:, 'FSCSKEY'].tolist())\n",
    "print(len(UniqueFSCS))\n",
    "\n",
    "#Pick 20% of FSCSKEYS randomly\n",
    "Numb_to_sel = int((.20*len(UniqueFSCS)))\n",
    "_20PctFSCSList = random.sample(UniqueFSCS, Numb_to_sel)\n",
    "\n",
    "#Get index of these FSCSKEYS from Location_Detail_DF_noNaN_noSpace\n",
    "Indexlist = list()\n",
    "for FSCSkey in _20PctFSCSList:\n",
    "    Indexfor_20PctFSCSList = Location_Detail_DF_noNaN_noSpace_DropOutliers[Location_Detail_DF_noNaN_noSpace_DropOutliers.FSCSKEY == FSCSkey].index\n",
    "    Indexlist.append(list(Indexfor_20PctFSCSList))\n",
    "flattened_Indexlist = [val for sublist in Indexlist for val in sublist]\n",
    "print(len(flattened_Indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF with SPECIFIC libraries removed.\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 60)\n",
    "\n",
    "X = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if I run with different random sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function that picks random 20% of libraries\n",
    "def Pick20PCTLIBS(Location_Detail_DF_noNaN_noSpace_DropOutliers, ReducedDF_noNaN_noSpace_DropOutliers_POPDIV):\n",
    "    ###GET random set of libraries to remove\n",
    "    Location_Detail_DF_noNaN_noSpace_DropOutliers = Location_Detail_DF_noNaN_noSpace_DropOutliers.reset_index(drop=True)\n",
    "    ReducedDF_noNaN_noSpace_DropOutliers_POPDIV = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV.reset_index(drop=True)\n",
    "\n",
    "    UniqueFSCS = set(Location_Detail_DF_noNaN_noSpace_DropOutliers.loc[:, 'FSCSKEY'].tolist())\n",
    "\n",
    "    #Pick 20% of FSCSKEYS randomly\n",
    "    Numb_to_sel = int((.20*len(UniqueFSCS)))\n",
    "    _20PctFSCSList = random.sample(UniqueFSCS, Numb_to_sel)\n",
    "\n",
    "    #Get index of these FSCSKEYS from Location_Detail_DF_noNaN_noSpace\n",
    "    Indexlist = list()\n",
    "    for FSCSkey in _20PctFSCSList:\n",
    "        Indexfor_20PctFSCSList = Location_Detail_DF_noNaN_noSpace_DropOutliers[Location_Detail_DF_noNaN_noSpace_DropOutliers.FSCSKEY == FSCSkey].index\n",
    "        Indexlist.append(list(Indexfor_20PctFSCSList))\n",
    "    flattened_Indexlist = [val for sublist in Indexlist for val in sublist]\n",
    "    return flattened_Indexlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Pick20PCTLIBS(Location_Detail_DF_noNaN_noSpace_DropOutliers, ReducedDF_noNaN_noSpace_DropOutliers_POPDIV)\n",
    "b = Pick20PCTLIBS(Location_Detail_DF_noNaN_noSpace_DropOutliers, ReducedDF_noNaN_noSpace_DropOutliers_POPDIV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define RF function\n",
    "#RF with SPECIFIC libraries removed.\n",
    "def RandomForestModel1(ReducedDF_noNaN_noSpace_DropOutliers_POPDIV, flattened_Indexlist):\n",
    "    rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 60)\n",
    "\n",
    "    X = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "    y = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV['Usage']\n",
    "\n",
    "    X_train = X.drop(X.index[flattened_Indexlist])\n",
    "    y_train = y.drop(y.index[flattened_Indexlist])\n",
    "    X_test = X.loc[flattened_Indexlist]\n",
    "    y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "    #Train model on TRAIN set\n",
    "    rfmodel.fit(X_train, y_train);\n",
    "\n",
    "    #Test model on TEST set (this is the right this to do)\n",
    "    predictions = rfmodel.predict(X_test)\n",
    "    errors = abs(predictions - y_test)\n",
    "    #df of predictions, labels and errors\n",
    "    df_TEST = pd.DataFrame()\n",
    "    df_TEST['Usage_val'] = y_test\n",
    "    df_TEST['Predictions'] = predictions\n",
    "    df_TEST['errors'] = (list(errors))\n",
    "\n",
    "    #Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "    predictions = rfmodel.predict(X_train)\n",
    "    errors = abs(predictions - y_train)\n",
    "    #df of predictions, labels and errors\n",
    "    df_TRAIN = pd.DataFrame()\n",
    "    df_TRAIN['Usage_val'] = y_train\n",
    "    df_TRAIN['Predictions'] = predictions\n",
    "    df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "    #Get feature importance\n",
    "    feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                       index = X_train.columns,\n",
    "                                        columns=['importance']).sort_values('importance', ascending=False)\n",
    "    print(feature_importances)\n",
    "\n",
    "    #Plot df_TEST Usage vs. Prediction:\n",
    "    # Data\n",
    "    x = df_TEST.iloc[:, 0]\n",
    "    y = df_TEST.iloc[:, 1]\n",
    "    # Fit with polyfit\n",
    "    b, m = polyfit(x, y, 1)\n",
    "    plt.plot(x, y, '.')\n",
    "    plt.plot(x, b + m * x, '-')\n",
    "    plt.show()\n",
    "\n",
    "    #Plot df_TRAIN Usage vs. Prediction:\n",
    "    # Data\n",
    "    x = df_TRAIN.iloc[:, 0]\n",
    "    y = df_TRAIN.iloc[:, 1]\n",
    "    # Fit with polyfit\n",
    "    b, m = polyfit(x, y, 1)\n",
    "    plt.plot(x, y, '.')\n",
    "    plt.plot(x, b + m * x, '-')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "    print(\"Score for Train set: \", rfmodel.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run random forest with 2 different sets of libs removed from training set\n",
    "RandomForestModel1(ReducedDF_noNaN_noSpace_DropOutliers_POPDIV, a)\n",
    "RandomForestModel1(ReducedDF_noNaN_noSpace_DropOutliers_POPDIV, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Good news is that feature importance is pretty consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if I run with different random state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not the most efficient way to do it, but will define new model to allow change in random state\n",
    "#At some point, I should combine these models\n",
    "def RandomForestModel2(ReducedDF_noNaN_noSpace_DropOutliers_POPDIV, flattened_Indexlist, number):\n",
    "    rfmodel = RandomForestRegressor(n_estimators = 100, random_state = number, max_depth = 60)\n",
    "\n",
    "    X = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "    y = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV['Usage']\n",
    "\n",
    "    X_train = X.drop(X.index[flattened_Indexlist])\n",
    "    y_train = y.drop(y.index[flattened_Indexlist])\n",
    "    X_test = X.loc[flattened_Indexlist]\n",
    "    y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "    #Train model on TRAIN set\n",
    "    rfmodel.fit(X_train, y_train);\n",
    "\n",
    "    #Test model on TEST set (this is the right this to do)\n",
    "    predictions = rfmodel.predict(X_test)\n",
    "    errors = abs(predictions - y_test)\n",
    "    #df of predictions, labels and errors\n",
    "    df_TEST = pd.DataFrame()\n",
    "    df_TEST['Usage_val'] = y_test\n",
    "    df_TEST['Predictions'] = predictions\n",
    "    df_TEST['errors'] = (list(errors))\n",
    "\n",
    "    #Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "    predictions = rfmodel.predict(X_train)\n",
    "    errors = abs(predictions - y_train)\n",
    "    #df of predictions, labels and errors\n",
    "    df_TRAIN = pd.DataFrame()\n",
    "    df_TRAIN['Usage_val'] = y_train\n",
    "    df_TRAIN['Predictions'] = predictions\n",
    "    df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "    #Get feature importance\n",
    "    feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                       index = X_train.columns,\n",
    "                                        columns=['importance']).sort_values('importance', ascending=False)\n",
    "    print(feature_importances)\n",
    "\n",
    "    #Plot df_TEST Usage vs. Prediction:\n",
    "    # Data\n",
    "    x = df_TEST.iloc[:, 0]\n",
    "    y = df_TEST.iloc[:, 1]\n",
    "    # Fit with polyfit\n",
    "    b, m = polyfit(x, y, 1)\n",
    "    plt.plot(x, y, '.')\n",
    "    plt.plot(x, b + m * x, '-')\n",
    "    plt.show()\n",
    "\n",
    "    #Plot df_TRAIN Usage vs. Prediction:\n",
    "    # Data\n",
    "    x = df_TRAIN.iloc[:, 0]\n",
    "    y = df_TRAIN.iloc[:, 1]\n",
    "    # Fit with polyfit\n",
    "    b, m = polyfit(x, y, 1)\n",
    "    plt.plot(x, y, '.')\n",
    "    plt.plot(x, b + m * x, '-')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "    print(\"Score for Train set: \", rfmodel.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run random forests with the two 20% lists defined above, but different random state than used before)\n",
    "RandomForestModel2(ReducedDF_noNaN_noSpace_DropOutliers_POPDIV, a, 10)\n",
    "RandomForestModel2(ReducedDF_noNaN_noSpace_DropOutliers_POPDIV, b, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Still super consistend in terms of R2 and features that come out as most important. Good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use fewer features in RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(feature_importances.index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run RF with top 10 features\n",
    "feats_to_include = list(feature_importances.index[0:10])\n",
    "\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 60)\n",
    "\n",
    "X = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV[feats_to_include]\n",
    "y = ReducedDF_noNaN_noSpace_DropOutliers_POPDIV['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Still good correlation. \n",
    "#Use just these 10 features in FLASK APP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sat Feb 2 2019 Change RF parameters a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB',\n",
       "       'EBOOK', 'ELMATEXP', 'FCAP_REV', 'DUPLI-HRS_OPEN', 'KIDPRO',\n",
       "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
       "       'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID',\n",
       "       'POPU-POPU_LSA', 'PRMATEXP', 'SALARIES', 'SCAP_REV', 'STAFFEXP-TOTEXP',\n",
       "       'STGVT', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
       "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'YAPRO', 'AUDIO', 'VIDEO', 'Usage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReducedDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete revenue columsn\n",
    "Revenue_cols_to_delete  = ['CAP_REV','FCAP_REV','LOCGVT','OCAP_REV','OTHINCM','SCAP_REV','TOTINCM']\n",
    "ReducedDF_NoRev = ReducedDF.drop(columns = Revenue_cols_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL', 'CENTLIB', 'EBOOK',\n",
       "       'ELMATEXP', 'DUPLI-HRS_OPEN', 'KIDPRO', 'LIBRARIAN-LIBRARIA', 'LOANFM',\n",
       "       'LOANTO', 'MASTER', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'POPU-POPU_LSA',\n",
       "       'PRMATEXP', 'SALARIES', 'STAFFEXP-TOTEXP', 'STGVT',\n",
       "       'TOTEXPCO-TOTEXPCOL', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO',\n",
       "       'TOTPEMP-TOTSTAFF', 'YAPRO', 'AUDIO', 'VIDEO', 'Usage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReducedDF_NoRev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([37237, 37239, 37242, 37243, 37244, 37245, 37247, 37248, 37250,\n",
      "            37251,\n",
      "            ...\n",
      "            46487, 46488, 46490, 46499, 46501, 46502, 46503, 46510, 46516,\n",
      "            46517],\n",
      "           dtype='int64', length=3322)\n",
      "Int64Index([37237, 37239, 37242, 37243, 37244, 37245, 37247, 37248, 37250,\n",
      "            37251,\n",
      "            ...\n",
      "            46487, 46488, 46490, 46499, 46501, 46502, 46503, 46510, 46516,\n",
      "            46517],\n",
      "           dtype='int64', length=3322)\n",
      "Int64Index([37237, 37239, 37242, 37243, 37244, 37245, 37247, 37248, 37250,\n",
      "            37251,\n",
      "            ...\n",
      "            46487, 46488, 46490, 46499, 46501, 46502, 46503, 46510, 46516,\n",
      "            46517],\n",
      "           dtype='int64', length=3322)\n",
      "Int64Index([37237, 37239, 37242, 37243, 37244, 37245, 37247, 37248, 37250,\n",
      "            37251,\n",
      "            ...\n",
      "            46487, 46488, 46490, 46499, 46501, 46502, 46503, 46510, 46516,\n",
      "            46517],\n",
      "           dtype='int64', length=3322)\n",
      "3322\n",
      "3322\n",
      "3322\n",
      "(65045, 30) (65045, 6)\n",
      "(61723, 30) (61723, 6)\n"
     ]
    }
   ],
   "source": [
    "#Delete columns with spaces.\n",
    "#4 columns - 'OTHOPEXP', 'SALARIES', 'STAFFEXP-TOTEXP', 'BENEFIT' - are objects, meaning some balues are non-numeric\n",
    "#It looks like some are left blank.\n",
    "ReducedDF_NoRev = ReducedDF_NoRev.reset_index(drop=True)\n",
    "Location_Detail_DF = Location_Detail_DF.reset_index(drop=True)\n",
    "\n",
    "print(ReducedDF_NoRev[ReducedDF_NoRev.OTHOPEXP == ' '].index)\n",
    "print(ReducedDF_NoRev[ReducedDF_NoRev.SALARIES == ' '].index)\n",
    "print(ReducedDF_NoRev[ReducedDF_NoRev['STAFFEXP-TOTEXP'] == ' '].index)\n",
    "print(ReducedDF_NoRev[ReducedDF_NoRev.BENEFIT == ' '].index)\n",
    "\n",
    "#Are all indices the same? YES. \n",
    "print(len(ReducedDF_NoRev[ReducedDF_NoRev.OTHOPEXP == ' '].index.intersection(ReducedDF_NoRev[ReducedDF_NoRev.SALARIES == ' '].index)))\n",
    "print(len(ReducedDF_NoRev[ReducedDF_NoRev.OTHOPEXP == ' '].index.intersection(ReducedDF_NoRev[ReducedDF_NoRev['STAFFEXP-TOTEXP'] == ' '].index)))\n",
    "print(len(ReducedDF_NoRev[ReducedDF_NoRev.OTHOPEXP == ' '].index.intersection(ReducedDF_NoRev[ReducedDF_NoRev.BENEFIT == ' '].index)))\n",
    "\n",
    "IndexToDropSpaces = ReducedDF_NoRev[ReducedDF_NoRev.OTHOPEXP == ' '].index\n",
    "\n",
    "ReducedDF_NoRev_noSpace = ReducedDF_NoRev.drop(IndexToDropSpaces)\n",
    "Location_Detail_DF_noSpace = Location_Detail_DF.drop(IndexToDropSpaces)\n",
    "print(ReducedDF_NoRev.shape, Location_Detail_DF.shape)\n",
    "print(ReducedDF_NoRev_noSpace.shape, Location_Detail_DF_noSpace.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(ReducedDF_NoRev_noSpace[ReducedDF_NoRev_noSpace.OTHOPEXP == ' '].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReducedDF_NoRev_noSpace['NaN total'] = ReducedDF_NoRev_noSpace.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61723, 31) (61386, 31)\n",
      "(61723, 6) (61386, 6)\n"
     ]
    }
   ],
   "source": [
    "#Delete all rows in which there are >6 NaN fields (20% of 30!)\n",
    "ReducedDF_NoRev_noSpace = ReducedDF_NoRev_noSpace.reset_index(drop = True)\n",
    "Location_Detail_DF_noSpace =Location_Detail_DF_noSpace.reset_index(drop = True)\n",
    "Delete7NaNRows = ReducedDF_NoRev_noSpace[ReducedDF_NoRev_noSpace['NaN total'] > 6].index\n",
    "ReducedDF_NoRev_noSpace_fewerNaN = ReducedDF_NoRev_noSpace.drop(Delete7NaNRows, axis = 0)\n",
    "Location_Detail_DF_noSpace_fewerNaN = Location_Detail_DF_noSpace.drop(Delete7NaNRows, axis = 0)\n",
    "print(ReducedDF_NoRev_noSpace.shape, ReducedDF_NoRev_noSpace_fewerNaN.shape)\n",
    "print(Location_Detail_DF_noSpace.shape, Location_Detail_DF_noSpace_fewerNaN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all columns to floats.\n",
    "ReducedDF_NoRev_noSpace_fewerNaN_float = ReducedDF_NoRev_noSpace_fewerNaN .astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing values with column means\n",
    "ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled = ReducedDF_NoRev_noSpace_fewerNaN_float.fillna(ReducedDF_NoRev_noSpace_fewerNaN_float.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled['NaN total'] = ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled.isnull().sum(axis=1)\n",
    "ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled['NaN total'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n",
      "(61386, 31) (61347, 31)\n",
      "(61386, 6) (61347, 6)\n"
     ]
    }
   ],
   "source": [
    "#Delete all rows where usage = 0 (there are very few libraries where this is true, so this is likely a mistake.)\n",
    "ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled = ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled.reset_index(drop = True)\n",
    "Location_Detail_DF_noSpace_fewerNaN = Location_Detail_DF_noSpace_fewerNaN.reset_index(drop = True)\n",
    "\n",
    "IdxUsage0 = ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled[ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled.Usage == 0].index\n",
    "ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0 = ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled.drop(IdxUsage0, axis = 0)\n",
    "Location_Detail_DF_noSpace_fewerNaN_noUsage0 = Location_Detail_DF_noSpace_fewerNaN.drop(IdxUsage0, axis = 0)\n",
    "#Check that it worked\n",
    "print(ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0[ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.Usage == 0].index)\n",
    "print(ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled.shape, ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.shape)\n",
    "print(Location_Detail_DF_noSpace_fewerNaN.shape, Location_Detail_DF_noSpace_fewerNaN_noUsage0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 112\n",
      "(61347, 31) (61235, 31)\n",
      "(61347, 6) (61235, 6)\n"
     ]
    }
   ],
   "source": [
    "### Remove Outliers, defined as anything over mean+5*SD\n",
    "\n",
    "ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0 = ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.reset_index(drop = True)\n",
    "Location_Detail_DF_noSpace_fewerNaN_noUsage0 = Location_Detail_DF_noSpace_fewerNaN_noUsage0.reset_index(drop = True)\n",
    "\n",
    "mean_plus_5xSD = ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.Usage.mean() + 5*ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.Usage.std()\n",
    "OutlierIdx = ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0[ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.Usage > mean_plus_5xSD].index\n",
    "print(\"Number of outliers:\", ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.loc[OutlierIdx, ['Usage']].shape[0])\n",
    "      \n",
    "ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0_DropOutliers=ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.drop(OutlierIdx)\n",
    "Location_Detail_DF_noSpace_fewerNaN_noUsage0_DropOutliers=Location_Detail_DF_noSpace_fewerNaN_noUsage0.drop(OutlierIdx)\n",
    "\n",
    "print(ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.shape, ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0_DropOutliers.shape)\n",
    "print(Location_Detail_DF_noSpace_fewerNaN_noUsage0.shape, Location_Detail_DF_noSpace_fewerNaN_noUsage0_DropOutliers.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run first RF with this DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61347, 30) (61347, 6)\n"
     ]
    }
   ],
   "source": [
    "DFforRF1 = ReducedDF_NoRev_noSpace_fewerNaN_NaNFilled_noUsage0.drop(columns = ['NaN total'])\n",
    "LocforRF1 = Location_Detail_DF_noSpace_fewerNaN_noUsage0\n",
    "print(DFforRF1.shape, LocforRF1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9402\n",
      "12304\n"
     ]
    }
   ],
   "source": [
    "# Run first RF [Did not divide any values by population yet!!]\n",
    "###GET random set of libraries to remove\n",
    "DFforRF1 = DFforRF1.reset_index(drop=True)\n",
    "LocforRF1 = LocforRF1.reset_index(drop=True)\n",
    "\n",
    "UniqueFSCS = set(LocforRF1.loc[:, 'FSCSKEY'].tolist())\n",
    "print(len(UniqueFSCS))\n",
    "\n",
    "#Pick 20% of FSCSKEYS randomly\n",
    "Numb_to_sel = int((.20*len(UniqueFSCS)))\n",
    "_20PctFSCSList = random.sample(UniqueFSCS, Numb_to_sel)\n",
    "\n",
    "#Get index of these FSCSKEYS from Location_Detail_DF_noNaN_noSpace\n",
    "Indexlist = list()\n",
    "for FSCSkey in _20PctFSCSList:\n",
    "    Indexfor_20PctFSCSList = LocforRF1[LocforRF1.FSCSKEY == FSCSkey].index\n",
    "    Indexlist.append(list(Indexfor_20PctFSCSList))\n",
    "flattened_Indexlist = [val for sublist in Indexlist for val in sublist]\n",
    "print(len(flattened_Indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF with SPECIFIC libraries removed.\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 60)\n",
    "\n",
    "X = DFforRF1.drop(columns = ['Usage'])\n",
    "y = DFforRF1['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
