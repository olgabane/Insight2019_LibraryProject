{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from numpy.polynomial.polynomial import polyfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep - all code condensed from Library_Week2_Notebook4_Strategy#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2014 (9305, 159) <class 'pandas.core.frame.DataFrame'>\n",
      "_2015 (9251, 159) <class 'pandas.core.frame.DataFrame'>\n",
      "_2016 (9252, 151) <class 'pandas.core.frame.DataFrame'>\n",
      "_2001 (9133, 108) <class 'pandas.core.frame.DataFrame'>\n",
      "_2002 (9141, 107) <class 'pandas.core.frame.DataFrame'>\n",
      "_2003 (9214, 110) <class 'pandas.core.frame.DataFrame'>\n",
      "_2004 (9210, 128) <class 'pandas.core.frame.DataFrame'>\n",
      "_2005 (9201, 131) <class 'pandas.core.frame.DataFrame'>\n",
      "_2006 (9211, 139) <class 'pandas.core.frame.DataFrame'>\n",
      "_2007 (9217, 148) <class 'pandas.core.frame.DataFrame'>\n",
      "_2008 (9284, 150) <class 'pandas.core.frame.DataFrame'>\n",
      "_2009 (9299, 152) <class 'pandas.core.frame.DataFrame'>\n",
      "_2010 (9308, 154) <class 'pandas.core.frame.DataFrame'>\n",
      "_2011 (9315, 157) <class 'pandas.core.frame.DataFrame'>\n",
      "_2012 (9305, 155) <class 'pandas.core.frame.DataFrame'>\n",
      "_2013 (9309, 157) <class 'pandas.core.frame.DataFrame'>\n",
      "_2000 (9078, 108) <class 'pandas.core.frame.DataFrame'>\n",
      "_1992 (8944, 83) <class 'pandas.core.frame.DataFrame'>\n",
      "_1993 (8929, 83) <class 'pandas.core.frame.DataFrame'>\n",
      "_1994 (8920, 83) <class 'pandas.core.frame.DataFrame'>\n",
      "_1995 (8981, 88) <class 'pandas.core.frame.DataFrame'>\n",
      "_1996 (8946, 90) <class 'pandas.core.frame.DataFrame'>\n",
      "_1997 (8968, 96) <class 'pandas.core.frame.DataFrame'>\n",
      "_1998 (8966, 102) <class 'pandas.core.frame.DataFrame'>\n",
      "_1999 (9048, 108) <class 'pandas.core.frame.DataFrame'>\n",
      "Original dfs:\n",
      "_2014 (9305, 159)\n",
      "_2015 (9251, 159)\n",
      "_2016 (9252, 151)\n",
      "_2001 (9133, 108)\n",
      "_2002 (9141, 107)\n",
      "_2003 (9214, 110)\n",
      "_2004 (9210, 128)\n",
      "_2005 (9201, 131)\n",
      "_2006 (9211, 139)\n",
      "_2007 (9217, 148)\n",
      "_2008 (9284, 150)\n",
      "_2009 (9299, 152)\n",
      "_2010 (9308, 154)\n",
      "_2011 (9315, 157)\n",
      "_2012 (9305, 155)\n",
      "_2013 (9309, 157)\n",
      "_2000 (9078, 108)\n",
      "_1992 (8944, 83)\n",
      "_1993 (8929, 83)\n",
      "_1994 (8920, 83)\n",
      "_1995 (8981, 88)\n",
      "_1996 (8946, 90)\n",
      "_1997 (8968, 96)\n",
      "_1998 (8966, 102)\n",
      "_1999 (9048, 108)\n",
      "\n",
      " Reduced dfs:\n",
      "_2014 (9305, 67)\n",
      "_2015 (9251, 70)\n",
      "_2016 (9252, 75)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 65)\n",
      "_2011 (9315, 65)\n",
      "_2012 (9305, 65)\n",
      "_2013 (9309, 66)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n",
      "_1999 (9048, 52)\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATCIR', 'ELMATEXP', 'ENDDATE',\n",
      "       'FCAP_REV', 'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN',\n",
      "       'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME',\n",
      "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID',\n",
      "       'PHONE', 'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'WIFISESS', 'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'EC_LO_OT', 'EC_ST', 'ELECCOLL',\n",
      "       'ELMATCIR', 'ELMATEXP', 'ENDDATE', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE',\n",
      "       'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX',\n",
      "       'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR',\n",
      "       'RSTATUS', 'SALARIES', 'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT',\n",
      "       'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL',\n",
      "       'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF',\n",
      "       'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS', 'WIFISESS', 'YAATTEN', 'YAPRO',\n",
      "       'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'EC_LO_OT', 'EC_ST', 'ELCONT',\n",
      "       'ELECCOLL', 'ELINFO', 'ELMATCIR', 'ELMATEXP', 'ENDDATE', 'FCAP_REV',\n",
      "       'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'PHYSCIR',\n",
      "       'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REAPLOCALE', 'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES',\n",
      "       'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT',\n",
      "       'TOTATTEN', 'TOTCIR', 'TOTCOLL', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM',\n",
      "       'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL',\n",
      "       'VIDEO_PH', 'ATTEND-VISITS', 'WIFISESS', 'YAATTEN', 'YAPRO',\n",
      "       'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'ERES_USR', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'ERES_USR', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ERES_USR', 'ESUBSCRP', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID', 'LIBNAME',\n",
      "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER', 'OBEREG',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'RSTATUS',\n",
      "       'SALARIES', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ERES_USR', 'ESUBSCRP', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE',\n",
      "       'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES', 'STABR', 'STAFFEXP-TOTEXP',\n",
      "       'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ERES_USR', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY',\n",
      "       'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM',\n",
      "       'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'RSTATUS',\n",
      "       'SALARIES', 'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT',\n",
      "       'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL',\n",
      "       'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT',\n",
      "       'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP',\n",
      "       'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP',\n",
      "       'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES',\n",
      "       'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT',\n",
      "       'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM',\n",
      "       'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT',\n",
      "       'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP',\n",
      "       'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP',\n",
      "       'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES',\n",
      "       'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT',\n",
      "       'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM',\n",
      "       'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE',\n",
      "       'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX',\n",
      "       'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR',\n",
      "       'RSTATUS', 'SALARIES', 'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT',\n",
      "       'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL',\n",
      "       'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CAP_REV', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'EBOOK',\n",
      "       'ELMATEXP', 'ENDDATE', 'ESUBSCRP', 'FCAP_REV', 'FSCSKEY', 'GEOCODE',\n",
      "       'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO',\n",
      "       'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE',\n",
      "       'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX',\n",
      "       'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'PRMATEXP', 'REFERENC-REFERENCE', 'REGBOR',\n",
      "       'RSTATUS', 'SALARIES', 'SCAP_REV', 'STABR', 'STAFFEXP-TOTEXP', 'STGVT',\n",
      "       'SUBSCRIP-SUBSCRIPT', 'TOTATTEN', 'TOTCIR', 'TOTEXPCO-TOTEXPCOL',\n",
      "       'TOTINCM', 'TOTOPEXP-TOTOPEXP1', 'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO',\n",
      "       'ATTEND-VISITS', 'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATEXP', 'ENDDATE', 'FCAP_REV',\n",
      "       'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR',\n",
      "       'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATEXP', 'ENDDATE', 'FCAP_REV',\n",
      "       'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR',\n",
      "       'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATEXP', 'ENDDATE', 'FCAP_REV',\n",
      "       'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN', 'KIDATTEN-KIDATTEND',\n",
      "       'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME', 'LIBRARIAN-LIBRARIA',\n",
      "       'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER', 'OBEREG', 'OCAP_REV',\n",
      "       'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'PITUSR',\n",
      "       'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO_DL', 'AUDIO_PH', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CAP_REV', 'CENTLIB', 'CITY', 'CNTY',\n",
      "       'C_LEGBAS-C_LEGBASE', 'EBOOK', 'ELMATCIR', 'ELMATEXP', 'ENDDATE',\n",
      "       'FCAP_REV', 'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN',\n",
      "       'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'KIDPRO', 'LIBID', 'LIBNAME',\n",
      "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCALE', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OCAP_REV', 'OTHINCM', 'OTHMATEX', 'OTHOPEXP', 'OTHPAID',\n",
      "       'PHONE', 'PITUSR', 'POPU-POPU_LSA', 'POPU_UND-POPU_UNDUP', 'PRMATEXP',\n",
      "       'REFERENC-REFERENCE', 'REGBOR', 'RSTATUS', 'SALARIES', 'SCAP_REV',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTATTEN',\n",
      "       'TOTCIR', 'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPRO', 'TOTPEMP-TOTSTAFF', 'VIDEO_DL', 'VIDEO_PH', 'ATTEND-VISITS',\n",
      "       'YAATTEN', 'YAPRO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'ERES_USR', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'FSCSKEY', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'FSCSKEY', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'FSCSKEY', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'ELACCEXP', 'ELMATEXP', 'ELMATS', 'ELSVCACC',\n",
      "       'FSCSKEY', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'ELACCEXP', 'ELMATEXP', 'ELMATS', 'ELSVCACC',\n",
      "       'FSCSKEY', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'ATTEND-VISITS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL',\n",
      "       'BRANLIB', 'CAPITAL', 'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE',\n",
      "       'DUPLI-HRS_OPEN', 'ELACCEXP', 'ELMATEXP', 'ELMATS', 'ELSVCACC',\n",
      "       'FSCSKEY', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR', 'STAFFEXP-TOTEXP',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'FSCSKEY', 'GEOCODE', 'GPTERMS', 'DUPLI-HRS_OPEN',\n",
      "       'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID', 'LIBNAME',\n",
      "       'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER', 'OBEREG',\n",
      "       'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "Index(['ADDRESS', 'AUDIO', 'BENEFIT', 'BKMOB', 'BKVOL', 'BRANLIB', 'CAPITAL',\n",
      "       'CENTLIB', 'CITY', 'CNTY', 'C_LEGBAS-C_LEGBASE', 'ELACCEXP', 'ELMATEXP',\n",
      "       'ELMATS', 'ELSVCACC', 'ERES_USR', 'FSCSKEY', 'GEOCODE', 'GPTERMS',\n",
      "       'DUPLI-HRS_OPEN', 'INETACC', 'KIDATTEN-KIDATTEND', 'KIDCIRCL', 'LIBID',\n",
      "       'LIBNAME', 'LIBRARIAN-LIBRARIA', 'LOANFM', 'LOANTO', 'LOCGVT', 'MASTER',\n",
      "       'OBEREG', 'OTHINCM', 'OTHOPEXP', 'OTHPAID', 'PHONE', 'POPU-POPU_LSA',\n",
      "       'POPU_UND-POPU_UNDUP', 'REFERENC-REFERENCE', 'RSTATUS', 'SALARIES',\n",
      "       'STABR', 'STAFFEXP-TOTEXP', 'STGVT', 'SUBSCRIP-SUBSCRIPT', 'TOTCIR',\n",
      "       'TOTEXPCO-TOTEXPCOL', 'TOTINCM', 'TOTOPEXP-TOTOPEXP1',\n",
      "       'TOTPEMP-TOTSTAFF', 'VIDEO', 'ATTEND-VISITS', 'ZIP-ZIP1'],\n",
      "      dtype='object')\n",
      "_2014 (9305, 67)\n",
      "_2015 (9251, 70)\n",
      "_2016 (9252, 75)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 65)\n",
      "_2011 (9315, 65)\n",
      "_2012 (9305, 65)\n",
      "_2013 (9309, 66)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n",
      "_1999 (9048, 52)\n",
      "_2014 (9305, 69)\n",
      "_2015 (9251, 72)\n",
      "_2016 (9252, 77)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 67)\n",
      "_2011 (9315, 67)\n",
      "_2012 (9305, 67)\n",
      "_2013 (9309, 68)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n",
      "_1999 (9048, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2014 (9305, 69)\n",
      "_2015 (9251, 72)\n",
      "_2016 (9252, 77)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 67)\n",
      "_2011 (9315, 67)\n",
      "_2012 (9305, 67)\n",
      "_2013 (9309, 68)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n",
      "_1999 (9048, 52)\n",
      "Reduced dfs:\n",
      "_2014 (9305, 69)\n",
      "_2015 (9251, 72)\n",
      "_2016 (9252, 77)\n",
      "_2001 (9133, 52)\n",
      "_2002 (9141, 52)\n",
      "_2003 (9214, 53)\n",
      "_2004 (9210, 56)\n",
      "_2005 (9201, 60)\n",
      "_2006 (9211, 61)\n",
      "_2007 (9217, 61)\n",
      "_2008 (9284, 62)\n",
      "_2009 (9299, 64)\n",
      "_2010 (9308, 67)\n",
      "_2011 (9315, 67)\n",
      "_2012 (9305, 67)\n",
      "_2013 (9309, 68)\n",
      "_2000 (9078, 52)\n",
      "_1992 (8944, 44)\n",
      "_1993 (8929, 44)\n",
      "_1994 (8920, 44)\n",
      "_1995 (8981, 49)\n",
      "_1996 (8946, 49)\n",
      "_1997 (8968, 49)\n",
      "_1998 (8966, 51)\n",
      "_1999 (9048, 52)\n",
      "\n",
      " Reduced and dropped useless dfs:\n",
      "_2014 (9305, 63)\n",
      "_2015 (9251, 66)\n",
      "_2016 (9252, 70)\n",
      "_2001 (9133, 47)\n",
      "_2002 (9141, 47)\n",
      "_2003 (9214, 48)\n",
      "_2004 (9210, 50)\n",
      "_2005 (9201, 54)\n",
      "_2006 (9211, 55)\n",
      "_2007 (9217, 55)\n",
      "_2008 (9284, 56)\n",
      "_2009 (9299, 58)\n",
      "_2010 (9308, 61)\n",
      "_2011 (9315, 61)\n",
      "_2012 (9305, 61)\n",
      "_2013 (9309, 62)\n",
      "_2000 (9078, 47)\n",
      "_1992 (8944, 39)\n",
      "_1993 (8929, 39)\n",
      "_1994 (8920, 39)\n",
      "_1995 (8981, 44)\n",
      "_1996 (8946, 44)\n",
      "_1997 (8968, 44)\n",
      "_1998 (8966, 46)\n",
      "_1999 (9048, 47)\n",
      "45\n",
      "(array([ 103,  446, 3560, 3782, 4470, 4478, 4739, 4767, 4768, 5177, 5214,\n",
      "       7393, 7395, 7400, 7401, 7403, 7407, 7408, 7409, 7410, 7412, 7414,\n",
      "       7416, 7421, 7422, 7424, 7425, 7427, 7428, 7429, 7431, 7433, 7434,\n",
      "       7435, 7436, 7438, 7441, 7442, 7444, 7445, 7447, 7448, 7449, 7450,\n",
      "       8559]),)\n",
      "45\n",
      "(array([ 103,  446, 3560, 3782, 4470, 4478, 4739, 4767, 4768, 5177, 5214,\n",
      "       7393, 7395, 7400, 7401, 7403, 7407, 7408, 7409, 7410, 7412, 7414,\n",
      "       7416, 7421, 7422, 7424, 7425, 7427, 7428, 7429, 7431, 7433, 7434,\n",
      "       7435, 7436, 7438, 7441, 7442, 7444, 7445, 7447, 7448, 7449, 7450,\n",
      "       8559]),)\n",
      "_2014 (9305, 63)\n",
      "_2015 (9251, 66)\n",
      "_2016 (9252, 70)\n",
      "_2001 (9133, 47)\n",
      "_2002 (9141, 47)\n",
      "_2003 (9214, 48)\n",
      "_2004 (9210, 50)\n",
      "_2005 (9201, 54)\n",
      "_2006 (9211, 55)\n",
      "_2007 (9217, 55)\n",
      "_2008 (9284, 56)\n",
      "_2009 (9299, 58)\n",
      "_2010 (9308, 61)\n",
      "_2011 (9315, 61)\n",
      "_2012 (9305, 61)\n",
      "_2013 (9309, 62)\n",
      "_2000 (9078, 47)\n",
      "_1992 (8944, 39)\n",
      "_1993 (8929, 39)\n",
      "_1994 (8920, 39)\n",
      "_1995 (8981, 44)\n",
      "_1996 (8946, 44)\n",
      "_1997 (8968, 44)\n",
      "_1998 (8966, 46)\n",
      "_1999 (9048, 47)\n"
     ]
    }
   ],
   "source": [
    "##CODE FROM NOTEBOOK: Library_Week2_Notebook4_Strategy#2\n",
    "#(Load and process data created in Library_Week2_Notebook4_Strategy#2)\n",
    "\n",
    "#Load data\n",
    "os.chdir(\"//Users/Olga/Documents/INSIGHT2019/Library data/AllPldData\")\n",
    "Files_in_folder = os.listdir()\n",
    "File_names = ['_2014', '_2015', '_2016', '_2001', '_2002', '_2003', \n",
    "              '_2004','_2005', '_2006', '_2007', '_2008', '_2009', \n",
    "              '_2010', '_2011', '_2012', '_2013', '_2000', '_1992', \n",
    "              '_1993', '_1994', '_1995', '_1996', '_1997', '_1998', '_1999']\n",
    "Files = []\n",
    "\n",
    "for filename in os.listdir():\n",
    "    if filename.endswith('csv'):\n",
    "        Files.append(pd.read_csv(filename, encoding = 'latin-1', low_memory = False))\n",
    "        \n",
    "#create dictionary of libraries labeled by year\n",
    "LibData_dict = {}\n",
    "for i in range(0, len(File_names)):\n",
    "    LibData_dict[File_names[i]] = Files[i]\n",
    "    \n",
    "for k, v in LibData_dict.items():\n",
    "    print(k, v.shape, type(LibData_dict[k]))\n",
    "\n",
    "ColumnsPresent = pd.DataFrame(index=range(0),columns=range(0))\n",
    "\n",
    "for k, v in LibData_dict.items():\n",
    "    dataframe_to_join = pd.DataFrame({k: list(v.columns)})\n",
    "    ColumnsPresent = ColumnsPresent.join(dataframe_to_join, how = 'outer')\n",
    "\n",
    "#Rearrange columns in dataframe. \n",
    "cols = ColumnsPresent.columns.tolist()\n",
    "cols_rearr = cols[17:] + cols[0:17]\n",
    "cols_rearr2 = cols_rearr[0:8] + cols_rearr[24:25] + cols_rearr[11:24] + cols_rearr[8:11]\n",
    "\n",
    "ColumnsPresent_rearr = ColumnsPresent[cols_rearr2]\n",
    "\n",
    "ColumnsPresent_rearr_str = ColumnsPresent_rearr.applymap(str)\n",
    "ColumnsPresent_rearr_str = ColumnsPresent_rearr_str.reset_index(drop = True)\n",
    "\n",
    "#create DF of unique column values\n",
    "UniqueLabels = np.unique(ColumnsPresent_rearr_str.values)\n",
    "LabelsBoolDF = pd.DataFrame(UniqueLabels)\n",
    "\n",
    "#Create DF w boolean values for prensence of each unique value\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "for j in range(len(ColumnsPresent_rearr_str.columns)):\n",
    "    lis = []\n",
    "    for i in range(len(UniqueLabels)):\n",
    "        lis.append(ColumnsPresent_rearr_str.iloc[:, j].isin([UniqueLabels[i]]).any())\n",
    "    LabelsBoolDF[ColumnsPresent_rearr_str.columns[j]] = lis\n",
    "    \n",
    "## Sum all TRUE values. Value of 25 = all true\n",
    "sum_list = []\n",
    "for i in range(LabelsBoolDF.shape[0]):\n",
    "    sum_list.append(sum(LabelsBoolDF.iloc[i, 1:26]))\n",
    "\n",
    "#Label OK in Notes\n",
    "LabelsBoolDF['TRUE SUM'] = sum_list\n",
    "LabelsBoolDF['Notes'] = \"\"\n",
    "LabelsBoolDF.loc[LabelsBoolDF[LabelsBoolDF['TRUE SUM']==25].index, 'Notes'] = \"OK\"\n",
    "\n",
    "#rows in which not all TRUE (meaning this column/data does not exist for all year)\n",
    "MissingData = LabelsBoolDF[LabelsBoolDF['TRUE SUM']!=25].index.tolist()\n",
    "\n",
    "#Column names to keep\n",
    "#LabelsBoolDF.loc[[0, 3:13, 15], 0].tolist()\n",
    "Cols_to_keep_idx = [0]\n",
    "Cols_to_keep_idx.extend(range(3,13))\n",
    "Cols_to_keep_idx.extend((16, 18, 20, 25, 26))\n",
    "Cols_to_keep_idx.extend(range(33, 49))\n",
    "Cols_to_keep_idx.extend((64, 135, 137, 138, 183))\n",
    "Cols_to_keep_idx.extend(range(185, 189))\n",
    "Cols_to_keep_idx.extend(range(191, 199))\n",
    "Cols_to_keep_idx.append(201)\n",
    "Cols_to_keep_idx.extend(range(206, 219))\n",
    "Cols_to_keep_idx.append(220)\n",
    "Cols_to_keep_idx.extend(range(222, 229))\n",
    "Cols_to_keep_idx.extend((230, 231, 237))\n",
    "Cols_to_keep_idx.extend(range(237, 256))\n",
    "Cols_to_keep_idx.extend(range(257, 260))\n",
    "Cols_to_keep_idx.extend((262, 263))\n",
    "\n",
    "Cols_to_keep_in_lib_dfs = LabelsBoolDF.iloc[Cols_to_keep_idx, 0].tolist()\n",
    "\n",
    "#Make dictionaries of smaller dataframe with Cols_to_keep_in_lib_dfs ONLY\n",
    "LibData_dict_reduced = dict()\n",
    "\n",
    "for k, v in LibData_dict.items():\n",
    "    LibData_dict_reduced[k] = pd.DataFrame()\n",
    "    for i in Cols_to_keep_in_lib_dfs:\n",
    "        if i in v:\n",
    "            TempList = list(v[i])\n",
    "            LibData_dict_reduced[k][i] = TempList\n",
    "\n",
    "#Did above loop do what I thought it did?\n",
    "print(\"Original dfs:\")\n",
    "for k, v in LibData_dict.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "print(\"\\n\", \"Reduced dfs:\")  \n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "\n",
    "#Combine col names that mean same thing in different years into single \n",
    "Names_to_combine = [\"ATTEND\", \"VISITS\",  \"C_LEGBAS\", \"C_LEGBASE\", \"DUPLI\", \"HRS_OPEN\", \"KIDATTEN\", \"KIDATTEND\", \n",
    "                    \"LIBRARIAN\", \"LIBRARIA\", \"POPU\", \"POPU_LSA\", \"POPU_UND\", \"POPU_UNDUP\", \"REFERENC\", \"REFERENCE\", \n",
    "                    \"STAFFEXP\", \"TOTEXP\", \"SUBSCRIP\", \"SUBSCRIPT\", \"TOTEXPCO\", \"TOTEXPCOL\", \n",
    "                    \"TOTOPEXP\", \"TOTOPEXP1\", \"TOTPEMP\", \"TOTSTAFF\", \"ZIP\", \"ZIP1\"]\n",
    "\n",
    "for i in range(0, len(Names_to_combine)-1, 2):\n",
    "    for k, v in LibData_dict_reduced.items(): \n",
    "        if Names_to_combine[i] in v:\n",
    "            name = Names_to_combine[i]\n",
    "            newname = Names_to_combine[i] + \"-\" + Names_to_combine[i+1]\n",
    "            v = v.rename(columns = {name: newname}, inplace = True)\n",
    "        elif Names_to_combine[i+1] in v:\n",
    "            name = Names_to_combine[i + 1]\n",
    "            newname = Names_to_combine[i] + \"-\" + Names_to_combine[i+1]\n",
    "            v = v.rename(columns = {name : newname}, inplace = True)\n",
    "\n",
    "#Check that above code worked - names changes\n",
    "for k, v in LibData_dict_reduced.items():\n",
    "    print(v.columns)\n",
    "\n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "\n",
    "#Columns to add to DFs\n",
    "\"AUDIO is divided into AUDIO_PH, AUDIO_DL starting 2010. Source: 2010 Documentation.\"\n",
    "\"VIDEO was later split into VIDEO_DL and VIDEO_PH.\"\n",
    "\n",
    "for k, v in LibData_dict_reduced.items(): \n",
    "    if \"AUDIO_DL\" in v:\n",
    "        v[\"AUDIO\"] = v['AUDIO_DL'] + v['AUDIO_PH']\n",
    "    if \"VIDEO_DL\" in v:\n",
    "        v[\"VIDEO\"] = v['VIDEO_DL'] + v['VIDEO_PH']\n",
    "\n",
    "#Check that # columns in some dataframes increased by 2 (expected)\n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "    \n",
    "#Create DF w boolean values for prensence of each unique value in REDUCED dataframes\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "\n",
    "#Get all unique columns present in LibData_dict_reduced dataframes. \n",
    "ColumnsPresent_reduced = pd.DataFrame(index=range(0),columns=range(0))\n",
    "\n",
    "for k, v in LibData_dict_reduced.items():\n",
    "    dataframe_to_join_reduced = pd.DataFrame({k: list(v.columns)})\n",
    "    ColumnsPresent_reduced = ColumnsPresent_reduced.join(dataframe_to_join_reduced, how = 'outer')\n",
    "\n",
    "ColumnsPresent_reduced_str = ColumnsPresent_reduced.applymap(str)\n",
    "ColumnsPresent_reduced_str = ColumnsPresent_reduced_str.reset_index(drop = True)\n",
    "    \n",
    "UniqueLabels_reduced = np.unique(ColumnsPresent_reduced_str.values)\n",
    "LabelsBoolDF_reduced = pd.DataFrame(UniqueLabels_reduced)\n",
    "\n",
    "#Create DF w boolean values for prensence of each unique value\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "for j in range(len(ColumnsPresent_reduced_str.columns)):\n",
    "    lis = []\n",
    "    for i in range(len(UniqueLabels_reduced)):\n",
    "        lis.append(ColumnsPresent_reduced_str.iloc[:, j].isin([UniqueLabels_reduced[i]]).any())\n",
    "    LabelsBoolDF_reduced[ColumnsPresent_reduced_str.columns[j]] = lis\n",
    "\n",
    "## Sum all TRUE values. Value of 25 = all true\n",
    "sum_list_reduced = []\n",
    "for i in range(LabelsBoolDF_reduced.shape[0]):\n",
    "    sum_list_reduced.append(sum(LabelsBoolDF_reduced.iloc[i, 1:26]))\n",
    "\n",
    "#Label OK in Notes\n",
    "LabelsBoolDF_reduced['TRUE SUM'] = sum_list_reduced\n",
    "LabelsBoolDF_reduced['Notes'] = \"\"\n",
    "LabelsBoolDF_reduced.loc[LabelsBoolDF_reduced[LabelsBoolDF_reduced['TRUE SUM']==25].index, 'Notes'] = \"OK\"\n",
    "\n",
    "All_Cols_in_Reduced_dfs = list(LabelsBoolDF_reduced[0])\n",
    "All_Cols_in_Reduced_dfs\n",
    "\n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "    \n",
    "#Delete columns deemed NOT useful for downstream analysis\n",
    "NonUsefulColstoDelete = ['ENDDATE', 'LIBID', 'PHONE', 'POPU_UND-POPU_UNDUP', 'RSTATUS', 'TOTCOLL', 'ZIP-ZIP1']\n",
    "\n",
    "LibData_dict_reduced_dropuseless = copy.deepcopy(LibData_dict_reduced)\n",
    "\n",
    "for k, v in LibData_dict_reduced_dropuseless.items(): \n",
    "    for i in range(len(NonUsefulColstoDelete)):\n",
    "        if NonUsefulColstoDelete[i] in v:\n",
    "            v.drop([NonUsefulColstoDelete[i]], axis = 1, inplace = True)\n",
    "            \n",
    "#Check quickly whether columns were dropped\n",
    "print(\"Reduced dfs:\")\n",
    "for k, v in LibData_dict_reduced.items():    \n",
    "    print(k, v.shape)\n",
    "\n",
    "print(\"\\n\", \"Reduced and dropped useless dfs:\")\n",
    "for k, v in LibData_dict_reduced_dropuseless.items():    \n",
    "    print(k, v.shape)\n",
    "\n",
    "#Get rid of negative values (replace w NaN)\n",
    "#For all DFs, replace (-1, -3, -9) values with NaN. \n",
    "LibData_dict_reduced_dropuseless_NegRepWNaN = copy.deepcopy(LibData_dict_reduced_dropuseless)\n",
    "\n",
    "for k, v in LibData_dict_reduced_dropuseless_NegRepWNaN.items():\n",
    "    v[(v == -1) | (v == -3) | (v == -9)] = np.nan\n",
    "    \n",
    "#See if this worked (looks like it did)\n",
    "print(sum(LibData_dict_reduced_dropuseless['_2014']['AUDIO_DL'] < 0))\n",
    "print(np.where((LibData_dict_reduced_dropuseless['_2014']['AUDIO_DL'] < 0) == True))\n",
    "\n",
    "print(sum(LibData_dict_reduced_dropuseless_NegRepWNaN['_2014']['AUDIO_DL'].isnull()))\n",
    "print(np.where(LibData_dict_reduced_dropuseless_NegRepWNaN['_2014']['AUDIO_DL'].isnull()))\n",
    "\n",
    "for k, v in LibData_dict_reduced_dropuseless_NegRepWNaN.items():\n",
    "    print(k, v.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2014 (9305, 64)\n",
      "_2015 (9251, 67)\n",
      "_2016 (9252, 71)\n",
      "_2001 (9133, 48)\n",
      "_2002 (9141, 48)\n",
      "_2003 (9214, 49)\n",
      "_2004 (9210, 51)\n",
      "_2005 (9201, 55)\n",
      "_2006 (9211, 56)\n",
      "_2007 (9217, 56)\n",
      "_2008 (9284, 57)\n",
      "_2009 (9299, 59)\n",
      "_2010 (9308, 62)\n",
      "_2011 (9315, 62)\n",
      "_2012 (9305, 62)\n",
      "_2013 (9309, 63)\n",
      "_2000 (9078, 48)\n",
      "_1992 (8944, 40)\n",
      "_1993 (8929, 40)\n",
      "_1994 (8920, 40)\n",
      "_1995 (8981, 45)\n",
      "_1996 (8946, 45)\n",
      "_1997 (8968, 45)\n",
      "_1998 (8966, 47)\n",
      "_1999 (9048, 48)\n",
      "_2014 (9305, 64)\n",
      "_2014 (9259, 64)\n",
      "_2015 (9251, 67)\n",
      "_2015 (9231, 67)\n",
      "_2016 (9252, 71)\n",
      "_2016 (9234, 71)\n",
      "_2001 (9133, 48)\n",
      "_2001 (9131, 48)\n",
      "_2002 (9141, 48)\n",
      "_2002 (9138, 48)\n",
      "_2003 (9214, 49)\n",
      "_2003 (9214, 49)\n",
      "_2004 (9210, 51)\n",
      "_2004 (9207, 51)\n",
      "_2005 (9201, 55)\n",
      "_2005 (9198, 55)\n",
      "_2006 (9211, 56)\n",
      "_2006 (9208, 56)\n",
      "_2007 (9217, 56)\n",
      "_2007 (9214, 56)\n",
      "_2008 (9284, 57)\n",
      "_2008 (9257, 57)\n",
      "_2009 (9299, 59)\n",
      "_2009 (9256, 59)\n",
      "_2010 (9308, 62)\n",
      "_2010 (9273, 62)\n",
      "_2011 (9315, 62)\n",
      "_2011 (9275, 62)\n",
      "_2012 (9305, 62)\n",
      "_2012 (9265, 62)\n",
      "_2013 (9309, 63)\n",
      "_2013 (9263, 63)\n",
      "_2000 (9078, 48)\n",
      "_2000 (9078, 48)\n",
      "_1992 (8944, 40)\n",
      "_1992 (8944, 40)\n",
      "_1993 (8929, 40)\n",
      "_1993 (8929, 40)\n",
      "_1994 (8920, 40)\n",
      "_1994 (8920, 40)\n",
      "_1995 (8981, 45)\n",
      "_1995 (8981, 45)\n",
      "_1996 (8946, 45)\n",
      "_1996 (8946, 45)\n",
      "_1997 (8968, 45)\n",
      "_1997 (8968, 45)\n",
      "_1998 (8966, 47)\n",
      "_1998 (8966, 47)\n",
      "_1999 (9048, 48)\n",
      "_1999 (9048, 48)\n"
     ]
    }
   ],
   "source": [
    "##CODE FROM NOTEBOOK: Library_Week2_Notebook4_Strategy#2\n",
    "#(Load and process data created in Library_Week2_Notebook4_Strategy#2)\n",
    "\n",
    "#Add usage column to each library\n",
    "for k, v in LibData_dict_reduced_dropuseless_NegRepWNaN.items():\n",
    "    v['Usage'] = v['ATTEND-VISITS']/v['POPU-POPU_LSA']\n",
    "    \n",
    "#Check that column was added\n",
    "for k, v in LibData_dict_reduced_dropuseless_NegRepWNaN.items():\n",
    "    print(k, v.shape)\n",
    "    \n",
    "#Remove all rows (libraries) for which Usage data missing for any window!\n",
    "LibwUsage = copy.deepcopy(LibData_dict_reduced_dropuseless_NegRepWNaN)\n",
    "for k, v in LibwUsage.items():\n",
    "    NullInd = v[v.Usage.isnull()].index\n",
    "    print(k, v.shape)\n",
    "    v = v.drop(NullInd, axis = 0)\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CODE FROM NOTEBOOK: Library_Week2_Notebook4_Strategy#2\n",
    "#(Load and process data created in Library_Week2_Notebook4_Strategy#2)\n",
    "\n",
    "#Create DF w boolean values for prensence of each unique value in REDUCED dataframes\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "\n",
    "#Get all unique columns present in LibData_dict_reduced dataframes. \n",
    "ColumnsPresent_reduced = pd.DataFrame(index=range(0),columns=range(0))\n",
    "\n",
    "for k, v in LibwUsage .items():\n",
    "    dataframe_to_join_reduced = pd.DataFrame({k: list(v.columns)})\n",
    "    ColumnsPresent_reduced = ColumnsPresent_reduced.join(dataframe_to_join_reduced, how = 'outer')\n",
    "    \n",
    "UniqueLabels_reduced = np.unique(ColumnsPresent_reduced_str.values)\n",
    "LabelsBoolDF_reduced = pd.DataFrame(UniqueLabels_reduced)\n",
    "\n",
    "#Create DF w boolean values for prensence of each unique value\n",
    "#THIS IS THE END GOAL OF THIS DATA\n",
    "for j in range(len(ColumnsPresent_reduced_str.columns)):\n",
    "    lis = []\n",
    "    for i in range(len(UniqueLabels_reduced)):\n",
    "        lis.append(ColumnsPresent_reduced_str.iloc[:, j].isin([UniqueLabels_reduced[i]]).any())\n",
    "    LabelsBoolDF_reduced[ColumnsPresent_reduced_str.columns[j]] = lis\n",
    "    \n",
    "#Reorder columns\n",
    "colnames = LabelsBoolDF_reduced.columns.tolist()\n",
    "col_order = colnames[0:1] + colnames[18:26] + colnames[17:18] + colnames[4:17] + colnames[1:4]\n",
    "LabelsBoolDF_reduced = LabelsBoolDF_reduced[col_order]\n",
    "\n",
    "## Sum all TRUE values. Value of 25 = all true\n",
    "sum_list_reduced_all = []\n",
    "for i in range(LabelsBoolDF_reduced.shape[0]):\n",
    "    sum_list_reduced_all.append(sum(LabelsBoolDF_reduced.iloc[i, 1:26]))\n",
    "    \n",
    "sum_list_reduced_ten = []\n",
    "for i in range(LabelsBoolDF_reduced.shape[0]):\n",
    "    sum_list_reduced_ten.append(sum(LabelsBoolDF_reduced.iloc[i, 15:26]))\n",
    "    \n",
    "sum_list_reduced_six = []\n",
    "for i in range(LabelsBoolDF_reduced.shape[0]):\n",
    "    sum_list_reduced_six.append(sum(LabelsBoolDF_reduced.iloc[i, 19:26]))\n",
    "\n",
    "#Label OK in Notes\n",
    "LabelsBoolDF_reduced['All_Years_Present'] = sum_list_reduced_all\n",
    "LabelsBoolDF_reduced['2006_2016_Present'] = sum_list_reduced_ten\n",
    "LabelsBoolDF_reduced['2010_2016_Present'] = sum_list_reduced_six\n",
    "\n",
    "#Based on this, I will use 2010-2016 DF. This has the most values.\n",
    "LabelsBoolDF_reduced.loc[29: ,:]\n",
    "\n",
    "#Rename first column\n",
    "LabelsBoolDF_reduced.rename(columns = {LabelsBoolDF_reduced.columns[0] : \"Col\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2014 (9305, 85)\n",
      "_2015 (9251, 85)\n",
      "_2016 (9252, 85)\n",
      "_2001 (9133, 85)\n",
      "_2002 (9141, 85)\n",
      "_2003 (9214, 85)\n",
      "_2004 (9210, 85)\n",
      "_2005 (9201, 85)\n",
      "_2006 (9211, 85)\n",
      "_2007 (9217, 85)\n",
      "_2008 (9284, 85)\n",
      "_2009 (9299, 85)\n",
      "_2010 (9308, 85)\n",
      "_2011 (9315, 85)\n",
      "_2012 (9305, 85)\n",
      "_2013 (9309, 85)\n",
      "_2000 (9078, 85)\n",
      "_1992 (8944, 85)\n",
      "_1993 (8929, 85)\n",
      "_1994 (8920, 85)\n",
      "_1995 (8981, 85)\n",
      "_1996 (8946, 85)\n",
      "_1997 (8968, 85)\n",
      "_1998 (8966, 85)\n",
      "_1999 (9048, 85)\n"
     ]
    }
   ],
   "source": [
    "##CODE FROM NOTEBOOK: Library_Week2_Notebook4_Strategy#2\n",
    "#(Load and process data created in Library_Week2_Notebook4_Strategy#2)\n",
    "\n",
    "#Make sure each df has all 83 columns. If column did not exist, add this column with NaN\n",
    "Unique_col_names_list = list(LabelsBoolDF_reduced['Col'])\n",
    "\n",
    "LibwUsage_2 = copy.deepcopy(LibwUsage)\n",
    "\n",
    "for k, v in LibwUsage_2.items():\n",
    "    for i in range(len(Unique_col_names_list)):\n",
    "        if Unique_col_names_list[i] not in v:\n",
    "            v[Unique_col_names_list[i]] = np.nan\n",
    "\n",
    "for k, v in LibwUsage_2.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2014 (9305, 64)\n",
      "_2015 (9251, 67)\n",
      "_2016 (9252, 71)\n",
      "_2001 (9133, 48)\n",
      "_2002 (9141, 48)\n",
      "_2003 (9214, 49)\n",
      "_2004 (9210, 51)\n",
      "_2005 (9201, 55)\n",
      "_2006 (9211, 56)\n",
      "_2007 (9217, 56)\n",
      "_2008 (9284, 57)\n",
      "_2009 (9299, 59)\n",
      "_2010 (9308, 62)\n",
      "_2011 (9315, 62)\n",
      "_2012 (9305, 62)\n",
      "_2013 (9309, 63)\n",
      "_2000 (9078, 48)\n",
      "_1992 (8944, 40)\n",
      "_1993 (8929, 40)\n",
      "_1994 (8920, 40)\n",
      "_1995 (8981, 45)\n",
      "_1996 (8946, 45)\n",
      "_1997 (8968, 45)\n",
      "_1998 (8966, 47)\n",
      "_1999 (9048, 48)\n"
     ]
    }
   ],
   "source": [
    "for k, v in LibwUsage.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is where I start to put last year's usage in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2009\n",
      "_2010\n",
      "_2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py:3778: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2012\n",
      "_2013\n",
      "_2014\n",
      "_2015\n"
     ]
    }
   ],
   "source": [
    "#Include previous usage in dataframes\n",
    "dftemp = pd.DataFrame()\n",
    "LibwUsage_addlastyearusage = dict()\n",
    "LibwUsage_copy = copy.deepcopy(LibwUsage_2)\n",
    "\n",
    "year_list = [str('_2009'), str('_2010'), str('_2011'), str('_2012'), str('_2013'), str('_2014'), str('_2015'), str('_2016')]\n",
    "for i in range(len(year_list)-1):\n",
    "    print(year_list[i])\n",
    "    dftemp = LibwUsage_copy[year_list[i]][['FSCSKEY', 'Usage']]\n",
    "    dftemp.rename(columns = {\"Usage\": \"Usage_PrevYear\"}, inplace = True)\n",
    "    LibwUsage_addlastyearusage[year_list[i+1]] = pd.merge(LibwUsage_copy[year_list[i+1]], dftemp, how = \"outer\", on = 'FSCSKEY')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65271, 86)\n",
      "Columns to delete: ['EC_LO_OT', 'EC_ST', 'ELACCEXP', 'ELCONT', 'ELECCOLL', 'ELINFO', 'ELMATCIR', 'ELMATS', 'ELSVCACC', 'ERES_USR', 'ESUBSCRP', 'INETACC', 'PHYSCIR', 'REAPLOCALE', 'TOTCOLL', 'WIFISESS']\n",
      "(65271, 86)\n",
      "(65271, 70)\n",
      "(65271, 63)\n",
      "(65271, 63) (65271, 61) (65271, 55) (65271, 161)\n"
     ]
    }
   ],
   "source": [
    "#Concatenate 2010-2016\n",
    "Concat201116 = pd.concat([LibwUsage_addlastyearusage['_2010'], LibwUsage_addlastyearusage['_2011'], LibwUsage_addlastyearusage['_2012'], LibwUsage_addlastyearusage['_2013'], LibwUsage_addlastyearusage['_2014'], LibwUsage_addlastyearusage['_2015'], LibwUsage_addlastyearusage['_2016']], axis=0, sort = False, join = \"outer\")\n",
    "print(Concat201116.shape)\n",
    "\n",
    "#rename first col\n",
    "LabelsBoolDF_reduced.rename(columns = {LabelsBoolDF_reduced.columns[0] : \"Col\"}, inplace = True)\n",
    "\n",
    "#Which features are not present in all 7 years?\n",
    "Index = LabelsBoolDF_reduced[LabelsBoolDF_reduced['2010_2016_Present'] != 7].index\n",
    "\n",
    "#Delete these features.\n",
    "Cols_to_delete = LabelsBoolDF_reduced.loc[Index, 'Col']\n",
    "\n",
    "#Delete from dataframe:\n",
    "Cols_to_del_list = list(Cols_to_delete[:-1])\n",
    "print(\"Columns to delete:\", Cols_to_del_list)\n",
    "\n",
    "#Delete columns\n",
    "Concat201116_INCOMPLETECOLSDEL = Concat201116.drop(columns = Cols_to_del_list)\n",
    "\n",
    "print(Concat201116.shape)\n",
    "print(Concat201116_INCOMPLETECOLSDEL.shape)\n",
    "\n",
    "#Nonuseful columns to delete from above:\n",
    "NonUsefulColstoDelete = ['ENDDATE', 'LIBID', 'PHONE', 'POPU_UND-POPU_UNDUP', 'RSTATUS', 'ZIP-ZIP1', 'nan']\n",
    "Concat201116_INCOMPLETECOLSDEL2 = Concat201116_INCOMPLETECOLSDEL.drop(columns = NonUsefulColstoDelete)\n",
    "print(Concat201116_INCOMPLETECOLSDEL2.shape)\n",
    "\n",
    "Concat201116_INCOMPLETECOLSDEL2.columns\n",
    "\n",
    "#I SHOULD CHECK THIS MORE CAREFULLY ON FRI!!\n",
    "\n",
    "#Columns to move to front, not use \n",
    "'ADDRESS', 'ATTEND-VISITS', 'CITY', 'CNTY', 'FSCSKEY', 'LIBNAME',   \n",
    "\n",
    "#Columns to delete prior to analysis\n",
    "'AUDIO_DL', 'AUDIO_PH'\n",
    "\n",
    "#Columns to categorize -- DELETE FOR NOW, but for tomorrow may was to categorize\n",
    "'GEOCODE', 'LOCALE', 'OBEREG', 'STABR', 'C_LEGBAS-C_LEGBASE'\n",
    "\n",
    "#Delete 'AUDIO_DL', 'AUDIO_PH'\n",
    "Concat201116_INCOMPLETECOLSDEL3 = Concat201116_INCOMPLETECOLSDEL2.drop(columns = ['AUDIO_DL', 'AUDIO_PH'])\n",
    "#Put 'ADDRESS', 'ATTEND-VISITS', 'CITY', 'CNTY', 'FSCSKEY', 'LIBNAME' in dif DF\n",
    "Location_Detail_DF = Concat201116_INCOMPLETECOLSDEL2[['ADDRESS', 'ATTEND-VISITS', 'CITY', 'CNTY', 'FSCSKEY', 'LIBNAME']]\n",
    "\n",
    "#NOW delete from concatDF\n",
    "Concat201116_INCOMPLETECOLSDEL4 = Concat201116_INCOMPLETECOLSDEL3.drop(columns = ['ADDRESS', 'ATTEND-VISITS', 'CITY', 'CNTY', 'FSCSKEY', 'LIBNAME'])\n",
    "\n",
    "#NOW concat 4 values\n",
    "Concat_w_dummies = pd.get_dummies(Concat201116_INCOMPLETECOLSDEL4, columns=['GEOCODE', 'LOCALE', 'OBEREG', 'STABR', 'C_LEGBAS-C_LEGBASE'], prefix=['GEOCODE', 'LOCALE', 'OBEREG', 'STABR', 'C_LEGBAS-C_LEGBASE'])\n",
    "\n",
    "print(Concat201116_INCOMPLETECOLSDEL2.shape, Concat201116_INCOMPLETECOLSDEL3.shape, Concat201116_INCOMPLETECOLSDEL4.shape, Concat_w_dummies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65271, 161)\n",
      "25350\n",
      "(39921, 161) (39921, 6)\n"
     ]
    }
   ],
   "source": [
    "###RESET INDEX for BOTH\n",
    "Concat_w_dummies = Concat_w_dummies.reset_index(drop = True)\n",
    "Location_Detail_DF = Location_Detail_DF.reset_index(drop = True)\n",
    "\n",
    "#25000 rows have NaN. Just DELETE These rows for now\n",
    "print(Concat_w_dummies.shape)\n",
    "print(len(pd.isnull(Concat_w_dummies).any(1).nonzero()[0]))\n",
    "\n",
    "idx = pd.isnull(Concat_w_dummies).any(1).nonzero()[0]\n",
    "\n",
    "#Delete_row_w_any_NaN\n",
    "Concat_w_dummies_noNaN = Concat_w_dummies.drop(idx)\n",
    "Location_Detail_DF_noNaN = Location_Detail_DF.drop(idx)\n",
    "\n",
    "print(Concat_w_dummies_noNaN.shape, Location_Detail_DF_noNaN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([37344, 37346, 37349, 37350, 37351, 37352, 37354, 37357, 37358,\n",
      "            37360,\n",
      "            ...\n",
      "            46594, 46595, 46597, 46606, 46608, 46609, 46610, 46617, 46623,\n",
      "            46624],\n",
      "           dtype='int64', length=3295)\n",
      "Int64Index([37344, 37346, 37349, 37350, 37351, 37352, 37354, 37357, 37358,\n",
      "            37360,\n",
      "            ...\n",
      "            46594, 46595, 46597, 46606, 46608, 46609, 46610, 46617, 46623,\n",
      "            46624],\n",
      "           dtype='int64', length=3295)\n",
      "Int64Index([37344, 37346, 37349, 37350, 37351, 37352, 37354, 37357, 37358,\n",
      "            37360,\n",
      "            ...\n",
      "            46594, 46595, 46597, 46606, 46608, 46609, 46610, 46617, 46623,\n",
      "            46624],\n",
      "           dtype='int64', length=3295)\n",
      "Int64Index([37344, 37346, 37349, 37350, 37351, 37352, 37354, 37357, 37358,\n",
      "            37360,\n",
      "            ...\n",
      "            46594, 46595, 46597, 46606, 46608, 46609, 46610, 46617, 46623,\n",
      "            46624],\n",
      "           dtype='int64', length=3295)\n",
      "3295\n",
      "3295\n",
      "3295\n",
      "(39921, 161) (39921, 6)\n",
      "(36626, 161) (36626, 6)\n"
     ]
    }
   ],
   "source": [
    "##CODE FROM NOTEBOOK: Library_Week2_Notebook4_Strategy#2\n",
    "#(Load and process data created in Library_Week2_Notebook4_Strategy#2)\n",
    "\n",
    "#Above, I found that 4 columns - 'OTHOPEXP', 'SALARIES', 'STAFFEXP-TOTEXP', 'BENEFIT' - are objects, meaning some balues are non-numeric\n",
    "#(Not shown explicitly in this notebook, but shown in previous notebook)\n",
    "#It looks like some are left blank.\n",
    "print(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN.OTHOPEXP == ' '].index)\n",
    "print(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN.SALARIES == ' '].index)\n",
    "print(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN['STAFFEXP-TOTEXP'] == ' '].index)\n",
    "print(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN.BENEFIT == ' '].index)\n",
    "\n",
    "#Are all indices the same? YES. \n",
    "print(len(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN.OTHOPEXP == ' '].index.intersection(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN.SALARIES == ' '].index)))\n",
    "print(len(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN.OTHOPEXP == ' '].index.intersection(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN['STAFFEXP-TOTEXP'] == ' '].index)))\n",
    "print(len(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN.OTHOPEXP == ' '].index.intersection(Concat_w_dummies_noNaN[Concat_w_dummies_noNaN.BENEFIT == ' '].index)))\n",
    "\n",
    "IndexToDropSpaces = Concat_w_dummies_noNaN[Concat_w_dummies_noNaN.OTHOPEXP == ' '].index\n",
    "\n",
    "Concat_w_dummies_noNaN_noSpace = Concat_w_dummies_noNaN.drop(IndexToDropSpaces)\n",
    "Location_Detail_DF_noNaN_noSpace = Location_Detail_DF_noNaN.drop(IndexToDropSpaces)\n",
    "print(Concat_w_dummies_noNaN.shape, Location_Detail_DF_noNaN.shape)\n",
    "print(Concat_w_dummies_noNaN_noSpace.shape, Location_Detail_DF_noNaN_noSpace.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run first RF. No outliers remove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the one very obvious outlier\n",
    "Idx = Concat_w_dummies_noNaN_noSpace[Concat_w_dummies_noNaN_noSpace.Usage > 1600].index\n",
    "\n",
    "Location_Detail_DF_noNaN_noSpace_1611dropped = Location_Detail_DF_noNaN_noSpace.drop(Idx)\n",
    "Concat_w_dummies_noNaN_noSpace_1611dropped = Concat_w_dummies_noNaN_noSpace.drop(Idx)\n",
    "\n",
    "Location_Detail_DF_noNaN_noSpace_1611dropped = Location_Detail_DF_noNaN_noSpace_1611dropped.reset_index(drop = True)\n",
    "Concat_w_dummies_noNaN_noSpace_1611dropped = Concat_w_dummies_noNaN_noSpace_1611dropped.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39921, 161)\n",
      "(36625, 161)\n",
      "(39921, 6)\n",
      "(36625, 6)\n",
      "6293\n",
      "7306\n"
     ]
    }
   ],
   "source": [
    "#Take out 20% of libraries randomly for testing\n",
    "\n",
    "import random\n",
    "print(Concat_w_dummies_noNaN.shape)\n",
    "print(Concat_w_dummies_noNaN_noSpace_1611dropped.shape)\n",
    "print(Location_Detail_DF_noNaN.shape)\n",
    "print(Location_Detail_DF_noNaN_noSpace_1611dropped.shape)\n",
    "\n",
    "Concat_w_dummies_noNaN_noSpace_1611dropped_ri = Concat_w_dummies_noNaN_noSpace_1611dropped.reset_index(drop = True)\n",
    "Location_Detail_DF_noNaN_noSpace_1611dropped = Location_Detail_DF_noNaN_noSpace_1611dropped.reset_index(drop = True)\n",
    "\n",
    "UniqueFSCS = set(Location_Detail_DF_noNaN_noSpace_1611dropped.loc[:, 'FSCSKEY'].tolist())\n",
    "print(len(UniqueFSCS))\n",
    "\n",
    "#Pick 20% of FSCSKEYS randomly\n",
    "Numb_to_sel = int((.20*len(UniqueFSCS)))\n",
    "_20PctFSCSList = random.sample(UniqueFSCS, Numb_to_sel)\n",
    "\n",
    "#Get index of these FSCSKEYS from Location_Detail_DF_noNaN_noSpace_ri\n",
    "Indexlist = list()\n",
    "for FSCSkey in _20PctFSCSList:\n",
    "    Indexfor_20PctFSCSList = Location_Detail_DF_noNaN_noSpace_1611dropped[Location_Detail_DF_noNaN_noSpace_1611dropped.FSCSKEY == FSCSkey].index\n",
    "    Indexlist.append(list(Indexfor_20PctFSCSList))\n",
    "\n",
    "flattened_Indexlist = [val for sublist in Indexlist for val in sublist]\n",
    "print(len(flattened_Indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 10, max_depth = 60)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_1611dropped_ri.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_1611dropped_ri['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is old library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model #1 - code condensed from Library_Week2_Notebook4_Strategy#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out 20% of libraries randomly for testing\n",
    "\n",
    "import random\n",
    "print(Concat_w_dummies_noNaN.shape)\n",
    "print(Concat_w_dummies_noNaN_noSpace.shape)\n",
    "print(Location_Detail_DF_noNaN.shape)\n",
    "print(Location_Detail_DF_noNaN_noSpace.shape)\n",
    "\n",
    "Concat_w_dummies_noNaN_noSpace_ri = Concat_w_dummies_noNaN_noSpace.reset_index(drop = True)\n",
    "Location_Detail_DF_noNaN_noSpace_ri = Location_Detail_DF_noNaN_noSpace.reset_index(drop = True)\n",
    "\n",
    "UniqueFSCS = set(Location_Detail_DF_noNaN_noSpace_ri.loc[:, 'FSCSKEY'].tolist())\n",
    "print(len(UniqueFSCS))\n",
    "\n",
    "#Pick 20% of FSCSKEYS randomly\n",
    "Numb_to_sel = int((.20*len(UniqueFSCS)))\n",
    "_20PctFSCSList = random.sample(UniqueFSCS, Numb_to_sel)\n",
    "\n",
    "#Get index of these FSCSKEYS from Location_Detail_DF_noNaN_noSpace_ri\n",
    "Indexlist = list()\n",
    "for FSCSkey in _20PctFSCSList:\n",
    "    Indexfor_20PctFSCSList = Location_Detail_DF_noNaN_noSpace_ri[Location_Detail_DF_noNaN_noSpace_ri.FSCSKEY == FSCSkey].index\n",
    "    Indexlist.append(list(Indexfor_20PctFSCSList))\n",
    "\n",
    "flattened_Indexlist = [val for sublist in Indexlist for val in sublist]\n",
    "print(len(flattened_Indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF with SPECIFIC libraries removed.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "predictions = rfmodel.predict(X_test)\n",
    "\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "#df of predictions, labels and errors\n",
    "df = pd.DataFrame(predictions, y_test)\n",
    "df['errors'] = (list(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfmodel.score(X_test, y_test))\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "# Sample data\n",
    "x = df.iloc[:, 0]\n",
    "y = df.iloc[:, 1]\n",
    "\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.xlim((0, 30))\n",
    "plt.ylim((0, 40))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can out-of-bag estimates substitute for a separate test set??\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, oob_score=True)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri['Usage']\n",
    "\n",
    "rfmodel.fit(X, y);\n",
    "\n",
    "oob_error = 1 - rfmodel.oob_score_\n",
    "\n",
    "predictions = rfmodel.predict(X)\n",
    "errors = abs(predictions - y)\n",
    "\n",
    "#df of predictions, labels and errors\n",
    "df = pd.DataFrame()\n",
    "df['Usage_val'] = y\n",
    "df['Predictions'] = predictions\n",
    "df['errors'] = (list(errors))\n",
    "\n",
    "#Probably not...oob error is VERY large. So training and testing on same dataset is not valid. (Although if I train and test on same set, then rsults are VERY good and capital is best predictor. Does this tell me anything??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfmodel.oob_score_)\n",
    "\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "# Data\n",
    "x = df.iloc[:, 0]\n",
    "y = df.iloc[:, 1]\n",
    "\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.xlim(0,200)\n",
    "plt.ylim(0,200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go back to RF with SPECIFIC libraries removed.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "predictions = rfmodel.predict(X_test)\n",
    "\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "#df of predictions, labels and errors\n",
    "df = pd.DataFrame()\n",
    "df['Usage_val'] = y_test\n",
    "df['Predictions'] = predictions\n",
    "df['errors'] = (list(errors))\n",
    "\n",
    "# Data\n",
    "x = df.iloc[:, 0]\n",
    "y = df.iloc[:, 1]\n",
    "\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.ylim(0, 40)\n",
    "plt.xlim(0, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Usage_bin'] = pd.qcut(df['Usage_val'], 4)\n",
    "df['Predictions_bin'] = pd.qcut(df['Predictions'], 4)\n",
    "\n",
    "df = df.reset_index(drop = True)\n",
    "df['Usage_bin_val'] = \"\"\n",
    "df['Predictions_bin_val'] = \"\"\n",
    "\n",
    "Usage_bin_list = df['Usage_bin'].unique()\n",
    "Predictions_bin_list = df['Predictions_bin'].unique()\n",
    "\n",
    "for j in range(df.shape[0]):\n",
    "    for i in range(4):\n",
    "        if str(df.loc[j, 'Usage_bin']) == str(Usage_bin_list[i]):\n",
    "            df.loc[j, 'Usage_bin_val'] = i+1 \n",
    "        if str(df.loc[j, 'Predictions_bin']) == str(Predictions_bin_list[i]):\n",
    "            df.loc[j, 'Predictions_bin_val'] = i+1 \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_equal = 0\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i, 'Usage_bin_val'] == df.loc[i, 'Predictions_bin_val']:\n",
    "        sum_equal = sum_equal+1\n",
    "print(sum_equal, df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Difference\"] = abs(df['Usage_bin_val'] - df['Predictions_bin_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='Difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error_pct'] = df['errors']/df['Usage_val']\n",
    "\n",
    "sum_error = 0\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i, 'error_pct'] <= .3:\n",
    "        sum_error = sum_error + 1\n",
    "print(sum_error, df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='Usage_val', bins = len(df['Usage_val'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF with SPECIFIC libraries removed, but POPU-POPU_LSA kept in.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri.drop(columns = ['Usage'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "predictions = rfmodel.predict(X_test)\n",
    "\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "#df of predictions, labels and errors\n",
    "df = pd.DataFrame()\n",
    "df['Usage_val'] = y_test\n",
    "df['Predictions'] = predictions\n",
    "df['errors'] = (list(errors))\n",
    "\n",
    "# Data\n",
    "x = df.iloc[:, 0]\n",
    "y = df.iloc[:, 1]\n",
    "\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#If I put back population, it looks like prediction improves a lot. BUT that's super unfair and I won't actually get anything meaningful out of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, try RF with train and test set (without removing specific libraries)\n",
    "\n",
    "#RF with SPECIFIC libraries removed, but POPU-POPU_LSA kept in.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri['Usage']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "rfmodel.fit(X_train, y_train);\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "#df of predictions, labels and errors\n",
    "df = pd.DataFrame()\n",
    "df['Usage_val'] = y_test\n",
    "df['Predictions'] = predictions\n",
    "df['errors'] = (list(errors))\n",
    "\n",
    "# Data\n",
    "x = df.iloc[:, 0]\n",
    "y = df.iloc[:, 1]\n",
    "\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)\n",
    "df['error_pct'] = df['errors']/df['Usage_val']\n",
    "\n",
    "sum_error = 0\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i, 'error_pct'] <= .35:\n",
    "        sum_error = sum_error + 1\n",
    "#print(sum_error, df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of usage 0 to get mean error\n",
    "df[df['Usage_val'] == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['Usage_val'] == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error_pct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['error_pct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['error_pct'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['error_pct'] > 8000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3502, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After 2:30 standup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I should run the model the right way. Take 20% of libraries out first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GET random set of libraries to remove\n",
    "UniqueFSCS = set(Location_Detail_DF_noNaN_noSpace_ri.loc[:, 'FSCSKEY'].tolist())\n",
    "print(len(UniqueFSCS))\n",
    "\n",
    "#Pick 20% of FSCSKEYS randomly\n",
    "Numb_to_sel = int((.20*len(UniqueFSCS)))\n",
    "_20PctFSCSList = random.sample(UniqueFSCS, Numb_to_sel)\n",
    "\n",
    "#Get index of these FSCSKEYS from Location_Detail_DF_noNaN_noSpace_ri\n",
    "Indexlist = list()\n",
    "for FSCSkey in _20PctFSCSList:\n",
    "    Indexfor_20PctFSCSList = Location_Detail_DF_noNaN_noSpace_ri[Location_Detail_DF_noNaN_noSpace_ri.FSCSKEY == FSCSkey].index\n",
    "    Indexlist.append(list(Indexfor_20PctFSCSList))\n",
    "flattened_Indexlist = [val for sublist in Indexlist for val in sublist]\n",
    "print(len(flattened_Indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF with SPECIFIC libraries removed, but POPU-POPU_LSA kept in.\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df.iloc[:, 0]\n",
    "y = df.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfmodel.score(X_test, y_test))\n",
    "print(rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_TEST['errors'].max())\n",
    "print(df_TRAIN['errors'].max())\n",
    "idx = df_TEST[df_TEST['errors'] > 1600].index\n",
    "print(idx)\n",
    "df_TEST.loc[33595, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take out usage outliers! (As shown above, there are definitely some!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Location_Detail_DF_noNaN_noSpace_ri.shape)\n",
    "print(Concat_w_dummies_noNaN_noSpace_ri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove anything 5SD's higher than mean?\n",
    "mean_plus_5xSD = Concat_w_dummies_noNaN_noSpace_ri.Usage.mean() + 5*Concat_w_dummies_noNaN_noSpace_ri.Usage.std()\n",
    "OutlierIdx = Concat_w_dummies_noNaN_noSpace_ri[Concat_w_dummies_noNaN_noSpace_ri.Usage > mean_plus_5xSD].index\n",
    "Concat_w_dummies_noNaN_noSpace_ri.loc[OutlierIdx, ['Usage']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually, for starters let's simply remove row 33595, with usage >1600\n",
    "Location_Detail_DF_noNaN_noSpace_ri_1611dropped = Location_Detail_DF_noNaN_noSpace_ri.drop([33595])\n",
    "Concat_w_dummies_noNaN_noSpace_ri_1611dropped = Concat_w_dummies_noNaN_noSpace_ri.drop([33595])\n",
    "\n",
    "Location_Detail_DF_noNaN_noSpace_ri_1611dropped = Location_Detail_DF_noNaN_noSpace_ri_1611dropped.reset_index(drop = True)\n",
    "Concat_w_dummies_noNaN_noSpace_ri_1611dropped = Concat_w_dummies_noNaN_noSpace_ri_1611dropped.reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GET random set of libraries to remove\n",
    "UniqueFSCS = set(Location_Detail_DF_noNaN_noSpace_ri_1611dropped.loc[:, 'FSCSKEY'].tolist())\n",
    "print(len(UniqueFSCS))\n",
    "\n",
    "#Pick 20% of FSCSKEYS randomly\n",
    "Numb_to_sel = int((.20*len(UniqueFSCS)))\n",
    "_20PctFSCSList = random.sample(UniqueFSCS, Numb_to_sel)\n",
    "\n",
    "#Get index of these FSCSKEYS from Location_Detail_DF_noNaN_noSpace_ri\n",
    "Indexlist = list()\n",
    "for FSCSkey in _20PctFSCSList:\n",
    "    Indexfor_20PctFSCSList = Location_Detail_DF_noNaN_noSpace_ri_1611dropped[Location_Detail_DF_noNaN_noSpace_ri_1611dropped.FSCSKEY == FSCSkey].index\n",
    "    Indexlist.append(list(Indexfor_20PctFSCSList))\n",
    "flattened_Indexlist = [val for sublist in Indexlist for val in sublist]\n",
    "print(len(flattened_Indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF with SPECIFIC libraries removed (and POPU-POPU_LSA removed too). [Single outlier dropped]\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_1611dropped.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_1611dropped['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_TEST['errors'].max())\n",
    "print(df_TRAIN['errors'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take out outliers!!\n",
    "#plot correlation map\n",
    "#email Jolene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TEST.loc[df_TEST[df_TEST['errors'] > 90].index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSCSKeys_Used_Above_Jan28_535pm = _20PctFSCSList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, try to remove anything 5SD's higher than mean.\n",
    "Concat_w_dummies_noNaN_noSpace_ri = Concat_w_dummies_noNaN_noSpace_ri.reset_index(drop = True)\n",
    "Location_Detail_DF_noNaN_noSpace_ri = Location_Detail_DF_noNaN_noSpace_ri.reset_index(drop = True)\n",
    "\n",
    "mean_plus_5xSD = Concat_w_dummies_noNaN_noSpace_ri.Usage.mean() + 5*Concat_w_dummies_noNaN_noSpace_ri.Usage.std()\n",
    "OutlierIdx = Concat_w_dummies_noNaN_noSpace_ri[Concat_w_dummies_noNaN_noSpace_ri.Usage > mean_plus_5xSD].index\n",
    "print(\"Number of outliers:\", Concat_w_dummies_noNaN_noSpace_ri.loc[OutlierIdx, ['Usage']].shape[0])\n",
    "      \n",
    "Concat_w_dummies_noNaN_noSpace_ri_DropOutliers=Concat_w_dummies_noNaN_noSpace_ri.drop(OutlierIdx)\n",
    "Location_Detail_DF_noNaN_noSpace_ri_DropOutliers=Location_Detail_DF_noNaN_noSpace_ri.drop(OutlierIdx)\n",
    "\n",
    "print(Concat_w_dummies_noNaN_noSpace_ri.shape, Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.shape)\n",
    "print(Location_Detail_DF_noNaN_noSpace_ri.shape, Location_Detail_DF_noNaN_noSpace_ri_DropOutliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GET random set of libraries to remove\n",
    "Location_Detail_DF_noNaN_noSpace_ri_DropOutliers = Location_Detail_DF_noNaN_noSpace_ri_DropOutliers.reset_index(drop=True)\n",
    "Concat_w_dummies_noNaN_noSpace_ri_DropOutliers = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.reset_index(drop=True)\n",
    "\n",
    "UniqueFSCS = set(Location_Detail_DF_noNaN_noSpace_ri_DropOutliers.loc[:, 'FSCSKEY'].tolist())\n",
    "print(len(UniqueFSCS))\n",
    "\n",
    "#Pick 20% of FSCSKEYS randomly\n",
    "Numb_to_sel = int((.20*len(UniqueFSCS)))\n",
    "_20PctFSCSList = random.sample(UniqueFSCS, Numb_to_sel)\n",
    "\n",
    "#Get index of these FSCSKEYS from Location_Detail_DF_noNaN_noSpace_ri\n",
    "Indexlist = list()\n",
    "for FSCSkey in _20PctFSCSList:\n",
    "    Indexfor_20PctFSCSList = Location_Detail_DF_noNaN_noSpace_ri_DropOutliers[Location_Detail_DF_noNaN_noSpace_ri_DropOutliers.FSCSKEY == FSCSkey].index\n",
    "    Indexlist.append(list(Indexfor_20PctFSCSList))\n",
    "flattened_Indexlist = [val for sublist in Indexlist for val in sublist]\n",
    "print(len(flattened_Indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF with SPECIFIC libraries removed (and POPU-POPU_LSA removed too).\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSCSKeys_Used_Above_Jan28_550pm = _20PctFSCSList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TEST['error_pct'] = df_TEST['errors']/df_TEST['Usage_val']\n",
    "\n",
    "sum_error = 0\n",
    "\n",
    "df_TEST = df_TEST.reset_index(drop = True)\n",
    "\n",
    "for i in range(df_TEST.shape[0]):\n",
    "    if df_TEST.loc[i, 'error_pct'] <= .5:\n",
    "        sum_error = sum_error + 1\n",
    "print(sum_error, df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to reduce overfitting in RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #1. Increase number of trees 10-fold\n",
    "#RF with SPECIFIC libraries removed (and POPU-POPU_LSA removed too).\n",
    "rfmodel = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))\n",
    "\n",
    "#Increasing estimators to 1000 did not do anything!\n",
    "#This is good to know. Took a while to run, so reduce back to 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #2. Reduce number of trees back to 100. Reduce max_features from auto (total features) to sqrt (sqrt(features))\n",
    "#RF with SPECIFIC libraries removed (and POPU-POPU_LSA removed too).\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_features = \"sqrt\")\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))\n",
    "\n",
    "#Doesn't really change much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #3. Reduce number of trees back to 100. Change max_features back to auto (total features). Reduce max_depth to 10 (this is a bit arbitrary and pretty low; can consider increasing).\n",
    "#RF with SPECIFIC libraries removed (and POPU-POPU_LSA removed too).\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 10)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))\n",
    "\n",
    "#Train set is worse, but test set is no better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #3. Reduce number of trees back to 100. Increase max_depth to 100.\n",
    "#RF with SPECIFIC libraries removed (and POPU-POPU_LSA removed too).\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 100)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))\n",
    "\n",
    "#Train set score jumps back up, test score still low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #4. Try max depth between 10 and 100\n",
    "#RF with SPECIFIC libraries removed (and POPU-POPU_LSA removed too).\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "    \n",
    "var_depth = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "for number in var_depth:\n",
    "    rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = number)\n",
    "    rfmodel.fit(X_train, y_train)\n",
    "    print(\"max_depth = \", number)\n",
    "    print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "    print(\"Score for Train set: \", rfmodel.score(X_train, y_train))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #5. Try max depth 40, max_features to sqrt (sqrt(features))\n",
    "#RF with SPECIFIC libraries removed (and POPU-POPU_LSA removed too).\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "    \n",
    "\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 40, max_features = \"sqrt\")\n",
    "rfmodel.fit(X_train, y_train)\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))\n",
    "\n",
    "#n_estimators = 100\n",
    "#max_features = 30-50% of the number of features (reducing may reduce overfitting; right now, it's # of features)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #6. Try max_features to sqrt (sqrt(features)), min_samples_leaf to 15\n",
    "#RF with SPECIFIC libraries removed (and POPU-POPU_LSA removed too).\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "    \n",
    "\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_features = \"sqrt\", min_samples_leaf = 15)\n",
    "rfmodel.fit(X_train, y_train)\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_s_l_list = [1, 2, 5, 10, 15]\n",
    "for number in m_s_l_list:\n",
    "    rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_features = \"auto\", min_samples_leaf = number)\n",
    "    rfmodel.fit(X_train, y_train)\n",
    "    print(\"min_sample_leaf: \", number)\n",
    "    print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "    print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Back to original model. Is R2 score schewed by a few very bad error?\n",
    "#Left max_depth at 60, because R2 is about the same as if max_depth not set. \n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 60)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate R2 \n",
    "numerator = sum((df_TEST['Usage_val']-df_TEST['Predictions'])**2)\n",
    "denominator = sum((df_TEST['Usage_val']-df_TEST['Usage_val'].mean())**2)\n",
    "1-numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRY to classify instead (0, 1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_TEST['errors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_TEST.hist(column='errors'))\n",
    "print(df_TRAIN.hist(column='errors'))\n",
    "print(df_TEST.hist(column='Usage_val'))\n",
    "print(df_TRAIN.hist(column='Usage_val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_TEST[df_TEST['errors'] > 20].head())\n",
    "len(df_TEST[df_TEST['errors'] > 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate R2 for all where 'Usage_val' < 30.\n",
    "idx_high = df_TEST[df_TEST['Usage_val'] > 30].index\n",
    "df_TEST_lowuseonly = df_TEST.drop(idx_high)\n",
    "numerator = sum((df_TEST_lowuseonly['Usage_val']-df_TEST_lowuseonly['Predictions'])**2)\n",
    "denominator = sum((df_TEST_lowuseonly['Usage_val']-df_TEST_lowuseonly['Usage_val'].mean())**2)\n",
    "1-numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save libraries used\n",
    "my_list = _20PctFSCSList\n",
    "with open('your_file.txt', 'w') as f:\n",
    "    for item in my_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSCSKeys_Used_Above_Jan29_10am = _20PctFSCSList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are results similar when I pick random set of libraries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GET random set of libraries to remove\n",
    "Location_Detail_DF_noNaN_noSpace_ri_DropOutliers = Location_Detail_DF_noNaN_noSpace_ri_DropOutliers.reset_index(drop=True)\n",
    "Concat_w_dummies_noNaN_noSpace_ri_DropOutliers = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.reset_index(drop=True)\n",
    "\n",
    "UniqueFSCS = set(Location_Detail_DF_noNaN_noSpace_ri_DropOutliers.loc[:, 'FSCSKEY'].tolist())\n",
    "print(len(UniqueFSCS))\n",
    "\n",
    "#Pick 20% of FSCSKEYS randomly\n",
    "Numb_to_sel = int((.20*len(UniqueFSCS)))\n",
    "_20PctFSCSList = random.sample(UniqueFSCS, Numb_to_sel)\n",
    "\n",
    "#Get index of these FSCSKEYS from Location_Detail_DF_noNaN_noSpace_ri\n",
    "Indexlist = list()\n",
    "for FSCSkey in _20PctFSCSList:\n",
    "    Indexfor_20PctFSCSList = Location_Detail_DF_noNaN_noSpace_ri_DropOutliers[Location_Detail_DF_noNaN_noSpace_ri_DropOutliers.FSCSKEY == FSCSkey].index\n",
    "    Indexlist.append(list(Indexfor_20PctFSCSList))\n",
    "flattened_Indexlist = [val for sublist in Indexlist for val in sublist]\n",
    "print(len(flattened_Indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Back to original model. Is R2 score schewed by a few very bad error?\n",
    "#Left max_depth at 60, because R2 is about the same as if max_depth not set. \n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 60)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change random state - does this change predictions a lot?\n",
    "rfmodel = RandomForestRegressor(n_estimators = 100, random_state = 10, max_depth = 60)\n",
    "\n",
    "X = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers.drop(columns = ['Usage', 'POPU-POPU_LSA'])\n",
    "y = Concat_w_dummies_noNaN_noSpace_ri_DropOutliers['Usage']\n",
    "\n",
    "X_train = X.drop(X.index[flattened_Indexlist])\n",
    "y_train = y.drop(y.index[flattened_Indexlist])\n",
    "X_test = X.loc[flattened_Indexlist]\n",
    "y_test = y.loc[flattened_Indexlist]\n",
    "\n",
    "#Train model on TRAIN set\n",
    "rfmodel.fit(X_train, y_train);\n",
    "\n",
    "#Test model on TEST set (this is the right this to do)\n",
    "predictions = rfmodel.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "#df of predictions, labels and errors\n",
    "df_TEST = pd.DataFrame()\n",
    "df_TEST['Usage_val'] = y_test\n",
    "df_TEST['Predictions'] = predictions\n",
    "df_TEST['errors'] = (list(errors))\n",
    "\n",
    "#Test model on TRAIN set (this is technically the wrong thing to do, but still informative)\n",
    "predictions = rfmodel.predict(X_train)\n",
    "errors = abs(predictions - y_train)\n",
    "#df of predictions, labels and errors\n",
    "df_TRAIN = pd.DataFrame()\n",
    "df_TRAIN['Usage_val'] = y_train\n",
    "df_TRAIN['Predictions'] = predictions\n",
    "df_TRAIN['errors'] = (list(errors))\n",
    "\n",
    "#Get feature importance\n",
    "feature_importances = pd.DataFrame(rfmodel.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "#Plot df_TEST Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TEST.iloc[:, 0]\n",
    "y = df_TEST.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "#Plot df_TRAIN Usage vs. Prediction:\n",
    "# Data\n",
    "x = df_TRAIN.iloc[:, 0]\n",
    "y = df_TRAIN.iloc[:, 1]\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Score for Test set: \", rfmodel.score(X_test, y_test))\n",
    "print(\"Score for Train set: \", rfmodel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
